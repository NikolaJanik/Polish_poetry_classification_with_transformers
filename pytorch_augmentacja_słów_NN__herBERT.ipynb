{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_17sa8j9Cg4"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install sacremoses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLM5dKam9Vo6"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "import joblib\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import HerbertTokenizer, RobertaModel, AutoTokenizer, BertModel\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVqM-5ju0sLy"
      },
      "outputs": [],
      "source": [
        "class MLPClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "      self.fc1 = nn.Linear(768, 768*2)\n",
        "      self.fc2 = nn.Linear(768*2, 768*4)\n",
        "      self.out = nn.Linear(768*4, 4)\n",
        "      self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = x.view(x.size(0))\n",
        "      x = F.relu(self.fc1(x))\n",
        "      x = F.relu(self.fc2(x))\n",
        "      x = self.dropout(x)\n",
        "      x = self.out(x)\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZXB4PvlOGtQ"
      },
      "outputs": [],
      "source": [
        "model_NN = MLPClassifier()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INDz_1wVNqFX"
      },
      "outputs": [],
      "source": [
        "from torchsummary import summary\n",
        "summary(model_NN, (768,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gsi2jCABI_zh"
      },
      "outputs": [],
      "source": [
        "def common_compute(model, batch):\n",
        "    x, y = batch\n",
        "    logits = model(x)\n",
        "    loss = F.cross_entropy(logits, y)\n",
        "    return logits, loss, y\n",
        "\n",
        "def train_batch(model, optimizer, batch):\n",
        "    logits, loss, y = common_compute(model, batch)\n",
        "    _, predicted = torch.max(logits.data, -1)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    return loss, (predicted == y).sum().item()\n",
        "\n",
        "def validate_batch(model, batch):\n",
        "    logits, loss, y = common_compute(model, batch)\n",
        "    _, predicted = torch.max(logits.data, -1)\n",
        "    return loss, (predicted == y).sum().item()\n",
        "\n",
        "def test_batch(model, batch):\n",
        "    logits, loss, y = common_compute(model, batch)\n",
        "    _, predicted = torch.max(logits.data, -1)\n",
        "    return np.array(y).size, (predicted == y).sum().item(), loss, predicted, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rPT3MntJT-Z"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'weights.pth')\n",
        "loaded_net = MLPClassifier()\n",
        "loaded_net.load_state_dict(torch.load('weights.pth'))\n",
        "torch.save(model, 'model.pth')\n",
        "new_net = torch.load('model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJVOGWV1cs2K"
      },
      "outputs": [],
      "source": [
        "df = df_orginal\n",
        "model = herbert_klej\n",
        "fraction = 0.1\n",
        "alpha = 0.2\n",
        "n_epochs = 20\n",
        "scheme = 11\n",
        "test_size = 20\n",
        "val_size = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Y302eXuWka5"
      },
      "outputs": [],
      "source": [
        "os.mkdir(\"figs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KMi5BURO58-"
      },
      "outputs": [],
      "source": [
        "x = test[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zz_0AErVPCRS"
      },
      "outputs": [],
      "source": [
        "x2 = x.view(x.size(0), -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWTMheG1RYbx"
      },
      "outputs": [],
      "source": [
        "x2.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtLD7hgHY8Kt"
      },
      "outputs": [],
      "source": [
        "def get_confusion_matrix(model, X_test, y_test, model_name, data_type, classes):\n",
        "\n",
        "  cls =[]\n",
        "  for k in classes.keys():\n",
        "    cls.append(k)\n",
        "\n",
        "  y_pred = model.predict(X_test)\n",
        "  pred_labels=[]\n",
        "  for idx in range(len(y_pred)):\n",
        "    pred_label = np.argmax(y_pred[idx])\n",
        "    pred_labels.append(pred_label)\n",
        "\n",
        "  true_labels = y_test\n",
        "  score = accuracy_score(true_labels, pred_labels)\n",
        "  cm = confusion_matrix(true_labels, pred_labels, normalize='true')\n",
        "\n",
        "  #disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "  #disp.plot()\n",
        "  #disp.ax_.set_title(\"Model: {} | Data type: {} |  Acc: {}\".format(model_name, data_type, score))\n",
        "\n",
        "  #if classes is not None:\n",
        "      #tick_marks = np.arange(len(cls))\n",
        "      #plt.xticks(tick_marks, cls, rotation=45)\n",
        "      #plt.yticks(tick_marks, cls, rotation=50)\n",
        "\n",
        "  #plt.gcf().set_size_inches(10, 10)\n",
        "  #plt.savefig('/content/figs/{}_{}.png'.format(model_name, data_type), dpi=200)\n",
        "  #files.download('/content/figs/{}_{}.png'.format(model_name, data_type))\n",
        "\n",
        "  return score, cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UkHm_7f9coo"
      },
      "outputs": [],
      "source": [
        "def draw_learning_curve(history, data_type, key='accuracy'):\n",
        "\n",
        "  fig, ax = plt.subplots(1, 2, figsize=(12,6))\n",
        "  ax[0].plot(history.history[key])\n",
        "  ax[0].plot(history.history['val_'+ key] )\n",
        "  ax[0].set_ylabel(key.title())\n",
        "  ax[0].set_xlabel('Epoch')\n",
        "  ax[0].legend(['train', 'val'])\n",
        "\n",
        "  ax[1].plot(history.history['loss'])\n",
        "  ax[1].plot(history.history['val_loss'] )\n",
        " # ax[1].set_ylim([0,1])\n",
        "  ax[1].set_ylabel('loss'.title())\n",
        "  ax[1].set_xlabel('Epoch')\n",
        "  ax[1].legend(['train', 'val'])\n",
        "  fig.suptitle('Learning curve | Data type: {}'.format(data_type))\n",
        "  plt.show()\n",
        "  #fig.savefig('/content/figs/learning_curve_{}.png'.format(data_type))\n",
        "  #files.download('/content/figs/learning_curve_{}.png'.format(data_type))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYBaBrbT9xTN"
      },
      "outputs": [],
      "source": [
        "def get_data_set(labels, df):\n",
        "  idxs = []\n",
        "  for label in labels:\n",
        "    idxs_for_label, = np.where(df['Label'] == label)\n",
        "    for idx in idxs_for_label:\n",
        "      idxs.append(idx)\n",
        "\n",
        "  new_df = df.iloc[idxs]\n",
        "  new_df = new_df.sample(frac = 1).reset_index(drop=True)\n",
        "  return new_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7MoiQjw-x5D"
      },
      "outputs": [],
      "source": [
        "def print_classes(df):\n",
        "  authors = {}\n",
        "  y = df['Label']\n",
        "  if len(df['Label'].unique()) < 8:\n",
        "    y = df ['Label'].factorize()[0]\n",
        "  num_classes = len(df['Label'].unique())\n",
        "  for label in range(0, num_classes):\n",
        "    i, = np.where(y == label)\n",
        "    authors['{}'.format(df['Author-short'][i[0]])] = label\n",
        "\n",
        "  return authors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24rzVy9CQTxc"
      },
      "outputs": [],
      "source": [
        "def make_embedding(df, model):\n",
        "\n",
        "  X_stack = []\n",
        "  model_name, tokenizer, model = model\n",
        "  embedded = {}\n",
        "  tokens = {}\n",
        "  num_idxs = df.shape[0]\n",
        "  for idx in tqdm(range(0,num_idxs)):\n",
        "    single_poem_input = df['Text'][idx]\n",
        "    inputs = tokenizer.batch_encode_plus([single_poem_input], max_length = 512, padding=\"longest\", add_special_tokens=True, return_tensors=\"pt\",)\n",
        "    single_poem_output = model(**inputs)\n",
        "    X_single_poem = single_poem_output[0][:,0,:].detach().numpy()\n",
        "    X_stack.append(X_single_poem[0])\n",
        "\n",
        "    embedded[idx] = X_single_poem[0], df['Label'][idx]\n",
        "\n",
        "  df_embedded = pd.DataFrame.from_dict(embedded,  orient='index', columns=['embedding', 'label'])\n",
        "\n",
        "  return df_embedded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xls271BrQf1F"
      },
      "outputs": [],
      "source": [
        "def normalize_data(X):\n",
        "\n",
        "  X_normalized = np.zeros((X.shape[0],X.shape[1]))\n",
        "\n",
        "  for idx in range(0,X.shape[0]):\n",
        "    X_normalized[idx,:] = (X[idx,:] - np.mean(X[idx,:]))/ np.std(X[idx,:])\n",
        "\n",
        "  return X_normalized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGHRxlCPYnV4"
      },
      "outputs": [],
      "source": [
        "def get_X_y(df, normalization=True):\n",
        "\n",
        "  X = np.stack(df['embedding'])\n",
        "  y = df['Label']\n",
        "  if(normalization==True):\n",
        "    X = normalize_data(X)\n",
        "\n",
        "  #jeśli jest mniej niż 8 klas:\n",
        "  if len(df['Label'].unique()) < 8:\n",
        "    y = df ['Label'].factorize()[0]\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "  X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
        "\n",
        "  print(X.shape)\n",
        "\n",
        "  return X, y, X_train, X_val, X_test, y_train, y_val, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_XQaNLoQh2h"
      },
      "outputs": [],
      "source": [
        "def get_X_y_train(df, normalization=True):\n",
        "\n",
        "  X = np.stack(df['embedding'])\n",
        "  y = df['Label']\n",
        "  if(normalization==True):\n",
        "    X = normalize_data(X)\n",
        "\n",
        "  #jeśli jest mniej niż 8 klas:\n",
        "  if len(df['Label'].unique()) < 8:\n",
        "    y = df ['Label'].factorize()[0]\n",
        "\n",
        "  X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "\n",
        "  print(X.shape)\n",
        "\n",
        "  return X, y, X_train, X_val, y_train, y_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcvYHaP1QmLl"
      },
      "outputs": [],
      "source": [
        "def get_X_y_test(df, normalization=True):\n",
        "  X = np.stack(df['embedding'])\n",
        "  y = df['Label']\n",
        "  if(normalization == True):\n",
        "    X = normalize_data(X)\n",
        "\n",
        "  #jeśli jest mniej niż 8 klas:\n",
        "  if len(df['Label'].unique()) < 8:\n",
        "    y = df ['Label'].factorize()[0]\n",
        "\n",
        "  return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcTyaa5T5UTn"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    no_punct=[words for words in text if words not in string.punctuation]\n",
        "    words_wo_punct=''.join(no_punct)\n",
        "    return words_wo_punct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBDAreY9R-fY"
      },
      "outputs": [],
      "source": [
        "def swap_words(df):\n",
        "\n",
        "  dict_new_text = {}\n",
        "\n",
        "  for idx in tqdm(range(0,len(df))):\n",
        "    text_split = df['Text'][idx].split()\n",
        "    to_swap_list = []\n",
        "    #n_pairs = int((len(text_split) * fraction)/2)\n",
        "   # if(n_pairs==0):\n",
        "    n_pairs = 1\n",
        "\n",
        "    for i in range(0,n_pairs):\n",
        "      idx_to_swap_1 = random.randrange(0, len(text_split))\n",
        "      idx_to_swap_2 = random.randrange(0, len(text_split))\n",
        "      if(idx_to_swap_1 != idx_to_swap_2):\n",
        "        word_to_swap_1 = text_split[idx_to_swap_1]\n",
        "        word_to_swap_2 = text_split[idx_to_swap_2]\n",
        "        text_split[idx_to_swap_1] = word_to_swap_2\n",
        "        text_split[idx_to_swap_2] = word_to_swap_1\n",
        "\n",
        "    new_text = ' '.join(text_split)\n",
        "\n",
        "    dict_new_text[idx] = new_text, df['Label'][idx], df['Author-short'][idx]\n",
        "\n",
        "  df_swap_words = pd.DataFrame.from_dict(dict_new_text, orient='index', columns=['Text', 'Label', 'Author-short'])\n",
        "  return df_swap_words\n",
        "\n",
        "# dzielenie każdego wiersza na fragmenty o długości 60 słów\n",
        "def cut_with_window(df, window_size, step_size):\n",
        "\n",
        "  dict_new_text = {}\n",
        "\n",
        "  for idx in tqdm(range(0,len(df))):\n",
        "    text_split = df['Text'][idx].split()\n",
        "    x = 0\n",
        "    y = window_size\n",
        "    i = 0\n",
        "    while x < len(text_split):\n",
        "      #y = x + window_size\n",
        "      new_text_arr = text_split[x:y]\n",
        "      new_text = ' '.join(new_text_arr)\n",
        "\n",
        "      i = i + 1\n",
        "      x = x + step_size\n",
        "      y = min(x + window_size, len(text_split)-1)\n",
        "      dict_new_text[idx, i] = new_text, df['Label'][idx], df['Author-short'][idx]\n",
        "\n",
        "  df_cut = pd.DataFrame.from_dict(dict_new_text,  orient='index', columns=['Text', 'Label', 'Author-short'])\n",
        "  return df_cut\n",
        "\n",
        "# usuwanie kielku randomowo wybranych słów z orginalnego wiersza\n",
        "def remove_words(df, fraction):\n",
        "\n",
        "  dict_new_text = {}\n",
        "\n",
        "  for idx in tqdm(range(0,len(df))):\n",
        "    text_split = df['Text'][idx].split()\n",
        "    f = int(fraction * len(text_split))\n",
        "    if(f==0):\n",
        "      f = 1\n",
        "    to_remove_list = []\n",
        "\n",
        "    for i in range(0,f):\n",
        "      to_remove = random.randrange(0, len(text_split))\n",
        "      to_remove_list.append(to_remove)\n",
        "\n",
        "    for word_idx in to_remove_list:\n",
        "      text_split[word_idx] = ' '\n",
        "\n",
        "    new_text = ' '.join(text_split)\n",
        "\n",
        "    dict_new_text[idx] = new_text, df['Label'][idx], df['Author-short'][idx]\n",
        "\n",
        "  df_remove = pd.DataFrame.from_dict(dict_new_text,  orient='index', columns=['Text', 'Label', 'Author-short'])\n",
        "  return df_remove\n",
        "\n",
        "\n",
        "# podział wiersza na pół\n",
        "def cut_in_half(df):\n",
        "\n",
        "  new_dict_1 = {}\n",
        "  new_dict_2 = {}\n",
        "\n",
        "  for idx in tqdm(range(0,len(df))):\n",
        "\n",
        "    text_split = df['Text'][idx].split()\n",
        "    x = int(len(text_split)/2)\n",
        "    new_text_arr_1 = text_split[:x]\n",
        "    new_text_arr_2 = text_split[x:]\n",
        "    new_text_1 = \" \".join(new_text_arr_1)\n",
        "    new_text_2 = \" \".join(new_text_arr_2)\n",
        "\n",
        "    new_dict_1[idx] = new_text_1, df['Label'][idx], df['Author-short'][idx]\n",
        "    new_dict_2[idx] = new_text_2, df['Label'][idx], df['Author-short'][idx]\n",
        "\n",
        "  df_new_1 = pd.DataFrame.from_dict(new_dict_1,  orient='index', columns=['Text', 'Label', 'Author-short'])\n",
        "  df_new_2 = pd.DataFrame.from_dict(new_dict_2,  orient='index', columns=['Text', 'Label', 'Author-short'])\n",
        "\n",
        "  result = pd.concat([df_new_1, df_new_2])\n",
        "  df_cut = result.reset_index(drop=True)\n",
        "  df_cut\n",
        "  return df_cut\n",
        "\n",
        "#odbicie lustrzane wiersza\n",
        "def flip(df):\n",
        "  new_dict = {}\n",
        "\n",
        "  for idx in tqdm(range(0,len(df))):\n",
        "    text_split = df['Text'][idx].split()\n",
        "    text_split.reverse()\n",
        "    new_text = \" \".join(text_split)\n",
        "\n",
        "    new_dict[idx] = new_text, df['Label'][idx], df['Author-short'][idx]\n",
        "\n",
        "  df_flip = pd.DataFrame.from_dict(new_dict,  orient='index', columns=['Text', 'Label', 'Author-short'])\n",
        "  return df_flip\n",
        "\n",
        "def cut_and_flip(df, part):\n",
        "  new_dict = {}\n",
        "\n",
        "  for idx in tqdm(range(0,len(df))):\n",
        "    text_split = df['Text'][idx].split()\n",
        "    x = int(len(text_split)/2)\n",
        "    new_text_arr_1 = text_split[:x]\n",
        "    new_text_arr_2 = text_split[x:]\n",
        "    if(part==1):\n",
        "      new_text_arr_1.reverse()\n",
        "    if(part==2):\n",
        "      new_text_arr_2.reverse()\n",
        "    if(part==3):\n",
        "      new_text_arr_1.reverse()\n",
        "      new_text_arr_2.reverse()\n",
        "    new_text_arr = new_text_arr_1 + new_text_arr_2\n",
        "    new_text = \" \".join(new_text_arr)\n",
        "\n",
        "    new_dict[idx] = new_text, df['Label'][idx], df['Author-short'][idx]\n",
        "\n",
        "  df_cut_and_flip = pd.DataFrame.from_dict(new_dict,  orient='index', columns=['Text', 'Label', 'Author-short'])\n",
        "  return df_cut_and_flip\n",
        "\n",
        "\n",
        "def remove_half(df):\n",
        "\n",
        "  new_dict = {}\n",
        "\n",
        "  for idx in tqdm(range(0,len(df))):\n",
        "\n",
        "    text_split = df['Text'][idx].split()\n",
        "    x = round(len(text_split)/2)\n",
        "    new_text_arr_1 = text_split[:x]\n",
        "    new_text_arr_2 = text_split[x:]\n",
        "    new_text_1 = \" \".join(new_text_arr_1)\n",
        "    new_text_2 = \" \".join(new_text_arr_2)\n",
        "    part_to_remove = random.choices((0,1))\n",
        "    if(part_to_remove==0):\n",
        "      new_dict[idx] = new_text_1, df['Label'][idx], df['Author-short'][idx]\n",
        "    else:\n",
        "      new_dict[idx] = new_text_2, df['Label'][idx], df['Author-short'][idx]\n",
        "\n",
        "  df_remove_half = pd.DataFrame.from_dict(new_dict,  orient='index', columns=['Text', 'Label', 'Author-short'])\n",
        "\n",
        "  return df_remove_half\n",
        "\n",
        "def remove_part(df):\n",
        "\n",
        "  new_dict_1 = {}\n",
        "  new_dict_2 = {}\n",
        "\n",
        "  for idx in tqdm(range(0,len(df))):\n",
        "\n",
        "    text_split = df['Text'][idx].split()\n",
        "    x = round(len(text_split)/3)\n",
        "    new_text_arr_1 = text_split[:x]\n",
        "    new_text_arr_2 = text_split[x:2*x]\n",
        "    new_text_arr_3 = text_split[2*x:]\n",
        "    new_text_1 = \" \".join(new_text_arr_1)\n",
        "    new_text_2 = \" \".join(new_text_arr_2)\n",
        "    new_text_3 = \" \".join(new_text_arr_3)\n",
        "    part_to_remove = random.choices((0,2))\n",
        "    if(part_to_remove==0):\n",
        "      new_dict_1[idx] = new_text_2, df['Label'][idx], df['Author-short'][idx]\n",
        "      new_dict_2[idx] = new_text_3, df['Label'][idx], df['Author-short'][idx]\n",
        "    if(part_to_remove==1):\n",
        "      new_dict_1[idx] = new_text_1, df['Label'][idx], df['Author-short'][idx]\n",
        "      new_dict_2[idx] = new_text_3, df['Label'][idx], df['Author-short'][idx]\n",
        "    else:\n",
        "      new_dict_1[idx] = new_text_1, df['Label'][idx], df['Author-short'][idx]\n",
        "      new_dict_2[idx] = new_text_2, df['Label'][idx], df['Author-short'][idx]\n",
        "\n",
        "\n",
        "\n",
        "  df_new_1 = pd.DataFrame.from_dict(new_dict_1, orient='index', columns=['Text', 'Label', 'Author-short'])\n",
        "  df_new_2 = pd.DataFrame.from_dict(new_dict_2, orient='index', columns=['Text', 'Label', 'Author-short'])\n",
        "\n",
        "  result = pd.concat([df_new_1, df_new_2])\n",
        "  df_remove_part = result.reset_index(drop=True)\n",
        "  df_remove_part\n",
        "\n",
        "  return df_remove_part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPRWgrYLguuR"
      },
      "outputs": [],
      "source": [
        "def split_data(df_outer, split_size):\n",
        "  df = df_outer.copy(deep=True)\n",
        "  df_train = pd.DataFrame()\n",
        "  df_test = pd.DataFrame()\n",
        "  idxs_to_drop = []\n",
        "  idxs_for_test = []\n",
        "  n_classes = len(df['Label'].unique())\n",
        "\n",
        "  if n_classes == 8:\n",
        "    labels = df[\"Label\"].unique()\n",
        "    y = df['Label']\n",
        "  if n_classes < 8:\n",
        "    y = df['Label'].factorize()[0]\n",
        "    labels = np.arange(n_classes)\n",
        "\n",
        "  for n, label in enumerate(labels):\n",
        "    idxs_for_label, = np.where(y == label)\n",
        "    idxs_for_test = np.random.choice(idxs_for_label, size = split_size, replace=False)\n",
        "\n",
        "    for idx in idxs_for_test:\n",
        "      df_test = df_test.append(df.iloc[idx])\n",
        "      idxs_to_drop.append(idx)\n",
        "\n",
        "\n",
        "  for idx in range(0, len(df)):\n",
        "    if(idx not in idxs_to_drop):\n",
        "      df_train = df_train.append(df.iloc[idx])\n",
        "\n",
        "  df_train = df_train.reset_index(drop=True)\n",
        "  df_test = df_test.reset_index(drop=True)\n",
        "\n",
        "  if n_classes < 8:\n",
        "    df_test[\"Label\"] = df_test[\"Label\"].factorize()[0]\n",
        "    df_train[\"Label\"] = df_train[\"Label\"].factorize()[0]\n",
        "\n",
        "  return df_train, df_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42ynTtdEQtKw"
      },
      "outputs": [],
      "source": [
        "def make_experiment(model, df, test_size, val_size, data_type, scheme, fraction, alpha, n_realizations, normalization):\n",
        "\n",
        "  normalize = \"normalized.0\"\n",
        "  if(normalization==True):\n",
        "    normalize = \"normalized.1\"\n",
        "  model_name = model[0]\n",
        "\n",
        "  n_classes = len(df['Label'].unique())\n",
        "  CM = np.zeros((n_classes, n_classes, n_realizations))\n",
        "  scores = []\n",
        "  n_epochs = 20\n",
        "\n",
        "  train_loss_realizations = np.zeros((n_epochs, n_realizations))\n",
        "  train_acc_realizations = np.zeros((n_epochs, n_realizations))\n",
        "  val_loss_realizations = np.zeros((n_epochs, n_realizations))\n",
        "  val_acc_realizations = np.zeros((n_epochs, n_realizations))\n",
        "  for n in range(0, n_realizations):\n",
        "    # podział danych na dane testowe oraz treningowe i validacyjne. Dane testowe i validacyjne poddaję embedingowi i tworzę dla nich X, y\n",
        "    df_train_and_val, df_test = split_data(df, test_size)\n",
        "    df_train_to_augment, df_val = split_data(df_train_and_val, val_size)\n",
        "\n",
        "    embed_test = make_embedding(df_test, model)\n",
        "    df_test = pd.concat([df_test, embed_test['embedding']], axis=1)\n",
        "\n",
        "    embed_val = make_embedding(df_val, model)\n",
        "    df_val = pd.concat([df_val, embed_val['embedding']], axis=1)\n",
        "\n",
        "    #args = [fraction, alpha]\n",
        "    #df_train = generate_data(model, df_train_to_augment, scheme, args)\n",
        "\n",
        "    embed_train = make_embedding(df_train_to_augment, model)\n",
        "    df_train = pd.concat([df_train_to_augment, embed_train['embedding']], axis=1)\n",
        "\n",
        "    data_test = np.stack(df_test['embedding'])\n",
        "    label_test = df_test['Label'].values\n",
        "\n",
        "    data_val = np.stack(df_val['embedding'])\n",
        "    label_val = df_val['Label'].values\n",
        "\n",
        "    data_train = np.stack(df_train['embedding'])\n",
        "    label_train = df_train['Label'].values\n",
        "\n",
        "    test = TensorDataset(torch.FloatTensor(data_test),torch.LongTensor(label_test))\n",
        "    val = TensorDataset(torch.FloatTensor(data_val),torch.LongTensor(label_val))\n",
        "    train = TensorDataset(torch.FloatTensor(data_train),torch.LongTensor(label_train))\n",
        "\n",
        "    test_loader = DataLoader(test, batch_size=1024,shuffle=True)\n",
        "    val_loader = DataLoader(val, batch_size=1024,shuffle=True)\n",
        "    train_loader = DataLoader(train, batch_size=1024,shuffle=True)\n",
        "\n",
        "    model_NN = MLPClassifier()\n",
        "    optimizer = torch.optim.Adam(model_NN.parameters(), lr = 1e-4)\n",
        "\n",
        "    n_epochs = n_epochs\n",
        "    for epoch in range(n_epochs):\n",
        "      model_NN.train()\n",
        "      train_loss = []\n",
        "      train_acc = []\n",
        "      bar = tqdm(train, position=0, leave=False, desc='epoch %d'%epoch)\n",
        "      for batch in bar:\n",
        "        loss, acc = train_batch(model_NN, optimizer, batch)\n",
        "        train_loss.append(loss)\n",
        "        train_acc.append(acc)\n",
        "        avg_train_loss = torch.stack(train_loss).mean()\n",
        "        avg_train_loss = avg_train_loss.detach().numpy()\n",
        "        avg_train_acc = np.stack(train_acc).mean()\n",
        "      print('train_loss', avg_train_loss.item())\n",
        "      train_loss_realizations[epoch,n] = avg_train_loss\n",
        "      train_acc_realizations[epoch,n] = avg_train_acc\n",
        "\n",
        "      model_NN.eval()\n",
        "      with torch.no_grad():\n",
        "        val_loss = []\n",
        "        val_acc = []\n",
        "        for batch in val:\n",
        "          loss, acc = validate_batch(model_NN, batch)\n",
        "          val_loss.append(loss)\n",
        "          val_acc.append(acc)\n",
        "          avg_val_loss = torch.stack(val_loss).mean()\n",
        "          avg_val_loss = avg_val_loss.detach().numpy()\n",
        "          avg_val_acc = np.stack(val_acc).mean()\n",
        "        print('val_loss', avg_val_loss.item())\n",
        "\n",
        "        val_loss_realizations[epoch,n] = avg_val_loss\n",
        "        val_acc_realizations[epoch,n] = avg_val_acc\n",
        "\n",
        "    classes = print_classes(df_test)\n",
        "\n",
        "    bar = tqdm(test, position=0, leave=False, desc='test')\n",
        "    test_loss = []\n",
        "    true_labels = []\n",
        "    pred_labels = []\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in bar:\n",
        "            batch_size, batch_correct, loss, predicted, y = test_batch(model_NN, batch)\n",
        "            total += batch_size\n",
        "            correct += batch_correct\n",
        "            test_loss.append(loss)\n",
        "            true_labels.append(predicted)\n",
        "            pred_labels.append(y)\n",
        "        print('Acc: {}'.format(100 * float(correct) / total))\n",
        "        score = (100 * float(correct) / total)\n",
        "        cm = confusion_matrix(true_labels, pred_labels, normalize='true')\n",
        "        CM[:,:,n] = cm\n",
        "        scores.append(score)\n",
        "\n",
        "# dane do uśrednionej krzywej uczenia\n",
        "  val_loss_mean = np.mean(val_loss_realizations, axis=1)\n",
        "  train_loss_mean = np.mean(train_loss_realizations, axis=1)\n",
        "  val_acc_mean = np.mean(val_acc_realizations, axis=1)\n",
        "  train_acc_mean = np.mean(train_acc_realizations, axis=1)\n",
        "\n",
        "  val_loss_std = np.std(val_loss_realizations, axis=1)\n",
        "  train_loss_std = np.std(train_loss_realizations, axis=1)\n",
        "  val_acc_std = np.std(val_acc_realizations, axis=1)\n",
        "  train_acc_std = np.std(train_acc_realizations, axis=1)\n",
        "\n",
        "  dict_history = {\"val_loss_mean\":val_loss_mean,\n",
        "                  \"train_loss_mean\":train_loss_mean,\n",
        "                  \"val_acc_mean\":val_acc_mean,\n",
        "                  \"train_acc_mean\":train_acc_mean,\n",
        "                  \"val_loss_std\":val_loss_std,\n",
        "                  \"train_loss_std\":train_loss_std,\n",
        "                  \"val_acc_std\":val_acc_std,\n",
        "                  \"train_acc_std\":train_acc_std}\n",
        "\n",
        "# uśredniona krzywa uczenia\n",
        "  fontsize = 16\n",
        "  epoch_vec = np.arange(0,n_epochs)\n",
        "  fig, ax = plt.subplots(1,2,figsize = (12, 8))\n",
        "  clrs = sns.color_palette(\"flare\")\n",
        "  #ax[1].set_ylim([0,2])\n",
        "  ax[1].plot(epoch_vec, train_loss_mean, label = \"train\")\n",
        "  ax[1].fill_between(epoch_vec, train_loss_mean - train_loss_std, train_loss_mean + train_loss_std, alpha = 0.3, facecolor=clrs[4] )\n",
        "  ax[1].plot(val_loss_mean,  label = \"val\")\n",
        "  ax[1].fill_between(epoch_vec, val_loss_mean - val_loss_std, val_loss_mean + val_loss_std, alpha = 0.3, facecolor=clrs[4] )\n",
        "\n",
        "  ax[0].plot(epoch_vec, train_acc_mean,  label = \"train\")\n",
        "  ax[0].fill_between(epoch_vec, train_acc_mean - train_acc_std, train_acc_mean + train_acc_std, alpha = 0.3, facecolor=clrs[4])\n",
        "  ax[0].plot(val_acc_mean,  label = \"val\")\n",
        "  ax[0].fill_between(epoch_vec, val_acc_mean - val_acc_std, val_acc_mean + val_acc_std, alpha = 0.3, facecolor=clrs[4] )\n",
        "\n",
        "\n",
        "  ax[1].set_xlabel(\"Traning epoch\", fontsize=fontsize)\n",
        "  ax[1].set_ylabel(\"Loss\", fontsize=fontsize)\n",
        "  ax[1].set_yscale('log')\n",
        "  ax[0].set_xlabel(\"Traning epoch\", fontsize=fontsize)\n",
        "  ax[0].set_ylabel(\"Accuracy\", fontsize=fontsize)\n",
        "  ax[0].set_yscale('log')\n",
        "\n",
        "  ax[1].legend( fontsize = fontsize)\n",
        "  ax[0].legend( fontsize = fontsize)\n",
        "  fig.suptitle('Learning curve | Data type: {}'.format(data_type))\n",
        "\n",
        "  fig.savefig('/content/figs/avgr_learning_curve_{}_{}_{}_scheme_{}_fraction_{}_alpha_{}.png'.format(model_name, data_type, normalize, scheme, fraction, alpha))\n",
        "  files.download('/content/figs/avgr_learning_curve_{}_{}_{}_scheme_{}_fraction_{}_alpha_{}.png'.format(model_name, data_type, normalize, scheme, fraction, alpha))\n",
        "\n",
        "# uśredniona macierz konfuzji\n",
        "  CM_avrg = np.zeros((n_classes,n_classes))\n",
        "  CM_std = np.zeros((n_classes,n_classes))\n",
        "  score_avrg = np.mean(scores)\n",
        "\n",
        "  for i in range(0,n_classes):\n",
        "    for j in range(0,n_classes):\n",
        "      CM_avrg[i,j] = np.mean(CM[i,j,:])\n",
        "      CM_std[i,j] = np.std(CM[i,j,:])\n",
        "\n",
        "  classes = print_classes(df)\n",
        "  cls = []\n",
        "  for k in classes.keys():\n",
        "    cls.append(k)\n",
        "\n",
        "  tick_marks = np.arange(len(cls))\n",
        "  cms = {\"Average\": CM_avrg, \"Std\": CM_std}\n",
        "\n",
        "\n",
        "  fig2, axes = plt.subplots(1, 2, figsize=(20,10), sharey='row')\n",
        "\n",
        "  for i, (key, cm) in enumerate(cms.items()):\n",
        "\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=cls)\n",
        "    disp.plot(ax=axes[i], xticks_rotation=45)\n",
        "    disp.ax_.set_title(\"{} | Model: Neural Network | Data type: {} | Acc: {}\".format(key, data_type, round(score_avrg,2)))\n",
        "    disp.im_.colorbar.remove()\n",
        "    disp.ax_.set_xlabel('')\n",
        "    disp.ax_.set_ylabel('')\n",
        "\n",
        "\n",
        "  fig2.text(0.40, 0.1, 'Predicted label', ha='left')\n",
        "  plt.subplots_adjust(wspace=0.40, hspace=0.1)\n",
        "\n",
        "  fig2.colorbar(disp.im_, ax=axes)\n",
        "  plt.show()\n",
        "\n",
        "  plt.gcf().set_size_inches(10, 5)\n",
        "  fig2.savefig('/content/figs/avrg_neural_network_{}_{}_{}_scheme_{}_fraction_{}_alpha_{}.png'.format(model_name, data_type, normalize, scheme, fraction, alpha), dpi=200)\n",
        "  files.download('/content/figs/avrg_neural_network_{}_{}_{}_scheme_{}_fraction_{}_alpha_{}.png'.format(model_name, data_type, normalize, scheme, fraction, alpha))\n",
        "\n",
        "  return score_avrg, dict_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYJZ8dklSPte"
      },
      "outputs": [],
      "source": [
        "def generate_data(model, df_outer, scheme, args):\n",
        "\n",
        "  f, alpha = args\n",
        "  df = df_outer.copy(deep=True)\n",
        "  if(scheme==0):\n",
        "    df_new = cut_in_half(df)\n",
        "\n",
        "  if(scheme==1):\n",
        "    df_new = flip(df)\n",
        "\n",
        "  if(scheme==2):\n",
        "    df_new_1 = cut_in_half(df)\n",
        "    df_new_1 = flip(df_new_1)\n",
        "    df_new_2 = flip(df)\n",
        "    df_new = pd.concat([df_new_1, df_new_2]).reset_index(drop=True)\n",
        "\n",
        "  if(scheme==3):\n",
        "      df_new = remove_words(df, f)\n",
        "\n",
        "  if(scheme==4):\n",
        "      df_new_1 = cut_in_half(df)\n",
        "      df_new_1 = remove_words(df_new_1, f)\n",
        "      df_new_2 = remove_words(df, f)\n",
        "      df_new = pd.concat([df_new_1, df_new_2]).reset_index(drop=True)\n",
        "\n",
        "  if(scheme==5):\n",
        "      df_new_1 = remove_words(df, f)\n",
        "      df_new_1 = flip(df_new_1)\n",
        "      df_new_2 = remove_words(df, f)\n",
        "      df_new = pd.concat([df_new_1, df_new_2]).reset_index(drop=True)\n",
        "\n",
        "  if(scheme==6):\n",
        "      df_new_1 = cut_in_half(df)\n",
        "      df_new_1 = flip(df_new_1)\n",
        "      df_new_2 = remove_words(df, f)\n",
        "      df_new_2 = flip(df_new_2)\n",
        "      df_new_3 = remove_words(df, f)\n",
        "      df_new = pd.concat([df_new_1, df_new_2, df_new_3]).reset_index(drop=True)\n",
        "\n",
        "  if(scheme==7):\n",
        "\n",
        "      df_new_1 = cut_in_half(df)\n",
        "      df_new_1 = remove_words(df_new_1, f)\n",
        "      df_new_2 = cut_in_half(df)\n",
        "      df_new_2 = remove_words(df_new_2, f)\n",
        "      df_new_2 = flip(df_new_2)\n",
        "      df_new_3 = remove_words(df, f)\n",
        "      df_new_3 = flip(df_new_3)\n",
        "      df_new_4 = remove_words(df, f)\n",
        "      df_new = pd.concat([df_new_1, df_new_2, df_new_3, df_new_4]).reset_index(drop=True)\n",
        "\n",
        "  if(scheme==8):\n",
        "\n",
        "      df_new_1 = cut_and_flip(df,1)\n",
        "      df_new_2 = cut_and_flip(df,2)\n",
        "      df_new_3 = cut_and_flip(df,3)\n",
        "      df_new_4 = remove_words(df,f)\n",
        "      df_new_4 = flip(df_new_4)\n",
        "      df_new_5 = remove_words(df,f)\n",
        "      df_new = pd.concat([df_new_1, df_new_2, df_new_3, df_new_4, df_new_5]).reset_index(drop=True)\n",
        "\n",
        "  if(scheme==9):\n",
        "      df_new_1 = cut_and_flip(df,1)\n",
        "      df_new_1 = remove_words(df_new_1,f)\n",
        "      df_new_2 = cut_and_flip(df,2)\n",
        "      df_new_2 = remove_words(df_new_2,f)\n",
        "      df_new_3 = cut_and_flip(df,3)\n",
        "      df_new_3 =remove_words(df_new_3,f)\n",
        "      df_new_4 = remove_words(df,f)\n",
        "      df_new_4 = flip(df_new_4)\n",
        "      df_new_5 = remove_words(df,f)\n",
        "      df_new = pd.concat([df_new_1, df_new_2, df_new_3, df_new_4, df_new_5]).reset_index(drop=True)\n",
        "\n",
        "  if(scheme==10):\n",
        "      df_new_1 = df\n",
        "      df_new_aug = df.sample(round(len(df)*alpha)).reset_index(drop=True)\n",
        "      df_new_2 = remove_words(df_new_aug, f)\n",
        "      df_new_3 = flip(df_new_aug)\n",
        "      df_new_4 = cut_in_half(df_new_aug)\n",
        "      df_new = pd.concat([df_new_1, df_new_2, df_new_3, df_new_4]).reset_index(drop=True)\n",
        "\n",
        "\n",
        "  if(scheme==11):\n",
        "      df_new_1 = df\n",
        "      df_new_aug = df.sample(frac=alpha,axis=0).reset_index(drop=True)\n",
        "      x = int(len(df_new_aug)/4)\n",
        "      df_new_aug_1 = df_new_aug.iloc[:x]\n",
        "      df_new_aug_2 = df_new_aug.iloc[x:2*x]\n",
        "      df_new_aug_3 = df_new_aug.iloc[2*x:3*x]\n",
        "      df_new_aug_4 = df_new_aug.iloc[3*x:]\n",
        "      df_new_2 = remove_words(df_new_aug_1.reset_index(drop=True), f)\n",
        "      df_new_3 = flip(df_new_aug_2.reset_index(drop=True))\n",
        "      #df_new_4 = swap_words(df_new_aug_3.reset_index(drop=True))\n",
        "      df_new_5 = cut_in_half(df_new_aug_4.reset_index(drop=True))\n",
        "      #df_new = pd.concat([df_new_1, df_new_2, df_new_3, df_new_4, df_new_5]).reset_index(drop=True)\n",
        "      df_new = pd.concat([df_new_1, df_new_2, df_new_3,   df_new_5]).reset_index(drop=True)\n",
        "\n",
        "  if(scheme==12):\n",
        "      df_new_1 = df\n",
        "      df_new_aug = df.sample(frac=alpha,axis=0).reset_index(drop=True)\n",
        "      df_new_2 = remove_words(df_new_aug.sample(frac = 0.5, axis=0).reset_index(drop=True), f)\n",
        "      df_new_3 = flip(df_new_aug.sample(frac = 0.5, axis=0).reset_index(drop=True))\n",
        "      #df_new_4 = swap_words(df_new_aug.sample(frac = 0.33, axis=0).reset_index(drop=True))\n",
        "      df_new = pd.concat([df_new_1, df_new_2, df_new_3]).reset_index(drop=True)\n",
        "\n",
        "\n",
        "  if(scheme==13):\n",
        "      df_new_1 = df\n",
        "\n",
        "      df_new_aug = df.sample(frac=alpha,axis=0).reset_index(drop=True)\n",
        "      df_new_2 = remove_words(df_new_aug.sample(frac = 0.5, axis=0).reset_index(drop=True), f)\n",
        "      #df_new_3 = flip(df_new_aug.sample(frac = 0.33, axis=0).reset_index(drop=True))\n",
        "      df_new_4 = swap_words(df_new_aug.sample(frac = 0.5, axis=0).reset_index(drop=True))\n",
        "      df_new = pd.concat([df_new_1, df_new_2,  df_new_4]).reset_index(drop=True)\n",
        "\n",
        "\n",
        "\n",
        "  if(scheme==14):\n",
        "      df_new_1 = df\n",
        "\n",
        "      df_new_aug = df.sample(frac=alpha,axis=0).reset_index(drop=True)\n",
        "     # df_new_2 = remove_words(df_new_aug.sample(frac = 0.33, axis=0).reset_index(drop=True), f)\n",
        "      df_new_3 = flip(df_new_aug.sample(frac = 0.5, axis=0).reset_index(drop=True))\n",
        "      df_new_4 = swap_words(df_new_aug.sample(frac = 0.5, axis=0).reset_index(drop=True))\n",
        "      df_new = pd.concat([df_new_1, df_new_3, df_new_4]).reset_index(drop=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  df_embed = make_embedding(df_new, model)\n",
        "  df_new = pd.concat([df_new, df_embed[\"embedding\"]], axis=1)\n",
        "  df_new = df_new.sample(frac = 1).reset_index(drop=True)\n",
        "\n",
        "  return df_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXPq2-mxn4a7"
      },
      "outputs": [],
      "source": [
        "os.mkdir(\"figs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0i5K0o5-6Td2"
      },
      "outputs": [],
      "source": [
        "herbert_klej = [\"Herbert-klej\",\n",
        "                HerbertTokenizer.from_pretrained(\"allegro/herbert-klej-cased-tokenizer-v1\"),\n",
        "                RobertaModel.from_pretrained(\"allegro/herbert-klej-cased-v1\")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psQ-y7-wRUcz"
      },
      "outputs": [],
      "source": [
        "herbert_large = [\"Herbert-large\", HerbertTokenizer.from_pretrained(\"allegro/herbert-large-cased\"), RobertaModel.from_pretrained(\"allegro/herbert-large-cased\")]\n",
        "herbert_base = [\"Herbert-base\", HerbertTokenizer.from_pretrained(\"allegro/herbert-base-cased\"), RobertaModel.from_pretrained(\"allegro/herbert-base-cased\")]\n",
        "herbert_klej = [\"Herbert-klej\", HerbertTokenizer.from_pretrained(\"allegro/herbert-klej-cased-tokenizer-v1\"), RobertaModel.from_pretrained(\"allegro/herbert-klej-cased-v1\")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Is8A9eO4RYXj"
      },
      "outputs": [],
      "source": [
        "df_raw = pd.read_csv('/content/wiersze_do_BERT_Herbert_Miłosz.csv', \";\")\n",
        "df_raw .columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-NiIoQiRbQs"
      },
      "outputs": [],
      "source": [
        "df_raw  = df_raw.drop(columns = ['Unnamed: 9', 'Unnamed: 10', 'Unnamed: 11'])\n",
        "df_raw.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIWXKVbbY_cr"
      },
      "outputs": [],
      "source": [
        "df_raw.iloc[400:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyoxkvV6Ybgb"
      },
      "outputs": [],
      "source": [
        "df_raw = df_raw.drop(df_raw.index[400:])\n",
        "df_raw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_NXcd_cRm0B"
      },
      "outputs": [],
      "source": [
        "df_women = pd.concat([df_raw[\"Text\"],df_raw[\"Label\"],df_raw[\"Author-short\"]], axis=1)\n",
        "df_women = df_women[200:].reset_index(drop=True)\n",
        "df_men = pd.concat([df_raw[\"Text\"],df_raw[\"Label\"],df_raw[\"Author-short\"]], axis=1)\n",
        "df_men = df_men[:200].reset_index(drop=True)\n",
        "#df_women = df_women.sample(frac = 1).reset_index(drop=True)\n",
        "#df_men = df_men.sample(frac = 1).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgNS4unLRq13"
      },
      "outputs": [],
      "source": [
        "df_orginal = pd.DataFrame\n",
        "df_orginal = pd.concat([df_raw[\"Text\"],df_raw[\"Label\"],df_raw[\"Author-short\"]], axis=1)\n",
        "df_orginal = df_orginal.sample(frac = 1).reset_index(drop=True)\n",
        "df_orginal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFfZpCoMhx9t"
      },
      "outputs": [],
      "source": [
        "df = df_men\n",
        "model = herbert_klej\n",
        "data_type = 'men'\n",
        "\n",
        "n_realizations  = 10\n",
        "test_size       = 8    # number of samples per class for testing. Maximum: 50\n",
        "val_size        = 4    # number of samples per class for validating. Maximum: 50\n",
        "\n",
        "fraction_vec = np.array([0.1])\n",
        "scheme_vec  = np.array([11])\n",
        "alpha_vec   = np.array([0.0])\n",
        "\n",
        "accuracy_vs_scheme_vs_fraction = np.zeros((scheme_vec.shape[0], alpha_vec.shape[0]))\n",
        "dict_scores_all = []\n",
        "for idx_scheme, scheme in enumerate(scheme_vec):\n",
        "  for idx_alpha, alpha in enumerate(alpha_vec):\n",
        "    for idx_fraction, fraction in enumerate(fraction_vec):\n",
        "      score, dict_history_men = make_experiment(model, df, test_size, val_size, data_type, scheme, fraction, alpha, n_realizations, normalization=True)\n",
        "      dict_scores = {\"scheme\"     : scheme,\n",
        "                     \"fraction\"   : fraction,\n",
        "                     \"alpha\"      : alpha,\n",
        "                     \"score\"      : score\n",
        "                    }\n",
        "      dict_scores_all.append(dict_scores)\n",
        "      accuracy_vs_scheme_vs_fraction[idx_scheme, idx_alpha] = score\n",
        "      print(\"Scheme: {} | Fraction: {} | Score: {} | Alpha: {}\".format(scheme, fraction, score, alpha))\n",
        "\n",
        "    print(\"Acc matrix: \")\n",
        "    #print(accuracy_vs_scheme_vs_fraction)\n",
        "\n",
        "df_scores_all = pd.DataFrame(dict_scores_all)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ax.set_yscale('log')\n",
        "dict_history"
      ],
      "metadata": {
        "id": "90c40DAgD_7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_history_women"
      ],
      "metadata": {
        "id": "SEAe4ZsuopOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuhIDhmnDeM0"
      },
      "outputs": [],
      "source": [
        "accuracy_vs_scheme_vs_fraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2cEaUNI3JMR"
      },
      "outputs": [],
      "source": [
        "disp = ConfusionMatrixDisplay(confusion_matrix=accuracy_vs_scheme_vs_fraction)\n",
        "disp.plot()\n",
        "disp.ax_.set_title(\"Scheme and fraction matrix\")\n",
        "disp.ax_.set_xlabel('Fraction')\n",
        "disp.ax_.set_ylabel('Scheme')\n",
        "tick_marks = np.arange(len(fractions))\n",
        "plt.xticks(tick_marks, fractions)\n",
        "plt.yticks(tick_marks, schemes)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2To4Q6E_qQx"
      },
      "outputs": [],
      "source": [
        "X, y, X_train, X_test, y_train, y_test,  X_val, y_val = get_X_y(df)\n",
        "y_train = to_categorical(y_train)\n",
        "y_val = to_categorical(y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_MxCb5g2ut3"
      },
      "outputs": [],
      "source": [
        "classes = print_classes(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtJgfHpP_K77"
      },
      "outputs": [],
      "source": [
        "input_size = 1024\n",
        "num_classes = len(classes)\n",
        "batch_size = 512\n",
        "epochs = 200\n",
        "\n",
        "model_NN = Sequential([\n",
        "    Dense(input_size, input_dim=input_size, activation='relu'),\n",
        "    Dense(2*input_size, activation='relu'),\n",
        "   # Dropout(0.1),\n",
        "    #Dense(2*input_size, activation='relu'),\n",
        "    #Dropout(0.2),\n",
        "    #Dense(2*input_size, activation='relu'),\n",
        "    # Dense(2*input_size, activation='relu'),\n",
        "    # Dropout(0.2),\n",
        "    # Dense(2*input_size, activation='relu'),\n",
        "    # Dense(2*input_size, activation='relu'),\n",
        "   #  Dropout(0.2),\n",
        "     #Dense(2*input_size, activation='relu'),\n",
        "     Dense(4*input_size, activation='relu'),\n",
        "     Dropout(0.2),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "  ])\n",
        "\n",
        "model_NN.compile(loss='categorical_crossentropy', optimizer='Adam', metrics='accuracy')\n",
        "#callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=15)\n",
        "history = model_NN.fit(X_train, y_train, #callbacks=[callback],\n",
        "          batch_size=batch_size, epochs=epochs, verbose=1,\n",
        "          validation_data=(X_val, y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXIPOhxeQSh5"
      },
      "outputs": [],
      "source": [
        "get_confusion_matrix(model_NN, X_test, y_test, 'neural_network', 'all', classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMwLMJcb3QEP"
      },
      "outputs": [],
      "source": [
        "make_experiment(df, 'all', '2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpFSJa7imp09"
      },
      "outputs": [],
      "source": [
        "val_loss_mean = np.mean(val_loss_realizations, axis=1)\n",
        "train_loss_mean = np.mean(train_loss_realizations, axis=1)\n",
        "val_acc_mean = np.mean(val_acc_realizations, axis=1)\n",
        "train_acc_mean = np.mean(train_acc_realizations, axis=1)\n",
        "\n",
        "val_loss_std = np.std(val_loss_realizations, axis=1)\n",
        "train_loss_std = np.std(train_loss_realizations, axis=1)\n",
        "val_acc_std = np.std(val_acc_realizations, axis=1)\n",
        "train_acc_std = np.std(train_acc_realizations, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-7rw9ieoGWo"
      },
      "outputs": [],
      "source": [
        "fontsize = 16\n",
        "data_type = \"all\"\n",
        "epoch_vec = np.arange(0,epochs)\n",
        "fig, ax = plt.subplots(1,2,figsize = (12, 8))\n",
        "clrs = sns.color_palette(\"flare\")\n",
        "ax[1].set_ylim([0,2])\n",
        "ax[1].plot(epoch_vec, train_loss_mean, label = \"train\")\n",
        "ax[1].fill_between(epoch_vec, train_loss_mean - train_loss_std, train_loss_mean + train_loss_std, alpha = 0.3, facecolor=clrs[4] )\n",
        "ax[1].plot(val_loss_mean,  label = \"val\")\n",
        "ax[1].fill_between(epoch_vec, val_loss_mean - val_loss_std, val_loss_mean + val_loss_std, alpha = 0.3, facecolor=clrs[4] )\n",
        "\n",
        "ax[0].plot(epoch_vec, train_acc_mean,  label = \"train\")\n",
        "ax[0].fill_between(epoch_vec, train_acc_mean - train_acc_std, train_acc_mean + train_acc_std, alpha = 0.3, facecolor=clrs[4])\n",
        "ax[0].plot(val_acc_mean,  label = \"val\")\n",
        "ax[0].fill_between(epoch_vec, val_acc_mean - val_acc_std, val_acc_mean + val_acc_std, alpha = 0.3, facecolor=clrs[4] )\n",
        "\n",
        "\n",
        "ax[1].set_xlabel(\"Traning epoch\", fontsize=fontsize)\n",
        "ax[1].set_ylabel(\"Loss\", fontsize=fontsize)\n",
        "ax[0].set_xlabel(\"Traning epoch\", fontsize=fontsize)\n",
        "ax[0].set_ylabel(\"Accuracy\", fontsize=fontsize)\n",
        "\n",
        "ax[1].legend( fontsize = fontsize)\n",
        "ax[0].legend( fontsize = fontsize)\n",
        "fig.suptitle('Learning curve | Data type: {}'.format(data_type))\n",
        "\n",
        "fig.savefig('/content/figs/avgr_learning_curve_{}_remove_cut_3_remove.png'.format(data_type))\n",
        "files.download('/content/figs/avgr_learning_curve_{}_remove_cut_3_remove.png'.format(data_type))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjQpdOBCvgO4"
      },
      "outputs": [],
      "source": [
        "CM_avrg = np.zeros((n_classes,n_classes))\n",
        "CM_std = np.zeros((n_classes,n_classes))\n",
        "score_avrg = np.mean(scores)\n",
        "\n",
        "for i in range(0,n_classes):\n",
        "  for j in range(0,n_classes):\n",
        "    CM_avrg[i,j] = np.mean(CM[i,j,:])\n",
        "    CM_std[i,j] = np.std(CM[i,j,:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiQ5rqEZeFQd"
      },
      "outputs": [],
      "source": [
        "classes = print_classes(df)\n",
        "cls = []\n",
        "for k in classes.keys():\n",
        "  cls.append(k)\n",
        "\n",
        "tick_marks = np.arange(4)\n",
        "cms = {\"Average\": CM_avrg, \"Std\": CM_std}\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(20,10), sharey='row')\n",
        "\n",
        "for i, (key, cm) in enumerate(cms.items()):\n",
        "\n",
        "  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=cls)\n",
        "  disp.plot(ax=axes[i], xticks_rotation=45)\n",
        "  disp.ax_.set_title(\"{} | Model: Neural Network | Data type: {} | Acc: {}\".format(key, data_type, round(score_avrg,2)))\n",
        "  disp.im_.colorbar.remove()\n",
        "  disp.ax_.set_xlabel('')\n",
        "  disp.ax_.set_ylabel('')\n",
        "\n",
        "\n",
        "fig.text(0.40, 0.1, 'Predicted label', ha='left')\n",
        "plt.subplots_adjust(wspace=0.40, hspace=0.1)\n",
        "\n",
        "fig.colorbar(disp.im_, ax=axes)\n",
        "plt.show()\n",
        "\n",
        "plt.gcf().set_size_inches(10, 5)\n",
        "fig.savefig('/content/figs/avrg_neural_network_{}_remove_cut_3_remove.png'.format(data_type), dpi=200)\n",
        "files.download('/content/figs/avrg_neural_network_{}_remove_cut_3_remove.png'.format(data_type))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akXpQ9wZL-SF"
      },
      "outputs": [],
      "source": [
        "scores, cm = get_confusion_matrix(model_NN, X_test, y_test, 'neural_network', 'all', classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xc_4aQ60Mvuj"
      },
      "outputs": [],
      "source": [
        "draw_learning_curve(history, 'all')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1ZbLA058pBjTys-PhLyqITRtAi9ie_VKb",
      "authorship_tag": "ABX9TyM9xXEhNbpQfDSfBg7hPEzU"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}