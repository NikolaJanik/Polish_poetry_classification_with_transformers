{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOateELyjD7u9AGU87xsooi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NikolaJanik/Polish_poetry_classification_with_transformers/blob/main/ML_and_NN_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Install dependencies (for Google Colab)"
      ],
      "metadata": {
        "id": "l4cNjZhfbzfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers sacremoses --quiet\n",
        "!pip install xgboost lightgbm --quiet"
      ],
      "metadata": {
        "id": "CVT1VmO5bwOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Imports"
      ],
      "metadata": {
        "id": "0k90f2_6buBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from transformers import HerbertTokenizer, RobertaModel"
      ],
      "metadata": {
        "id": "Bgh6HWdBbrYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Load and prepare data\n"
      ],
      "metadata": {
        "id": "kA9RMcSWboZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_raw = pd.read_csv('polish_poetry.csv', sep=';')\n",
        "\n",
        "# Remove 'Unnamed' columns\n",
        "drop_cols = [col for col in df_raw.columns if 'Unnamed' in col]\n",
        "df_raw = df_raw.drop(columns=drop_cols)\n",
        "\n",
        "print(\"Number of poems:\", df_raw.shape[0])\n",
        "print(\"Number of classes:\", df_raw['Label'].nunique())\n",
        "print(\"Classes:\", df_raw['Author'].unique())"
      ],
      "metadata": {
        "id": "eu8TrOMIbl11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Tokenization and embedding function"
      ],
      "metadata": {
        "id": "Mm5PgWshbjyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_embedding(df, model_info):\n",
        "    model_name, tokenizer, model = model_info\n",
        "    embeddings = []\n",
        "    labels = []\n",
        "\n",
        "    for text, label in tqdm(zip(df['Text'], df['Label']), total=len(df)):\n",
        "        inputs = tokenizer(\n",
        "            text,\n",
        "            max_length=512,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        outputs = model(**inputs)\n",
        "        vector = outputs.last_hidden_state[:, 0, :].detach().numpy()[0]\n",
        "        embeddings.append(vector)\n",
        "        labels.append(label)\n",
        "\n",
        "    return np.array(embeddings), np.array(labels)"
      ],
      "metadata": {
        "id": "oYuJMVKYbg7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. HerBERT initialization"
      ],
      "metadata": {
        "id": "oDgyvP1Vbfb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "herbert = (\n",
        "    \"Herbert\",\n",
        "    HerbertTokenizer.from_pretrained(\"allegro/herbert-large-cased\"),\n",
        "    RobertaModel.from_pretrained(\"allegro/herbert-large-cased\")\n",
        ")"
      ],
      "metadata": {
        "id": "6EKUm3P3vKHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_all, y_all = make_embedding(df_raw, herbert)"
      ],
      "metadata": {
        "id": "J-Qum4YdbdNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Prepare X and y sets"
      ],
      "metadata": {
        "id": "iElwaJvHbcNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_splits(X, y, test_size=0.2, val_size=0.2):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, stratify=y)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_size, stratify=y_train)\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n"
      ],
      "metadata": {
        "id": "warB-uN7bVok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Machine learning model (Decision Tree)"
      ],
      "metadata": {
        "id": "pMaIqD6SbT1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_ml_model_multiple(X, y, model_class, model_name, n_realizations=20, test_size=0.2, val_size=0.2, **model_kwargs):\n",
        "    num_classes = len(np.unique(y))\n",
        "    cm_all = np.zeros((num_classes, num_classes, n_realizations))\n",
        "    scores_all = []\n",
        "\n",
        "    for r in range(n_realizations):\n",
        "        # Split dataset\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=test_size, stratify=y\n",
        "        )\n",
        "\n",
        "        # Build ML model\n",
        "        model = model_class(**model_kwargs)\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        # Accuracy and confusion matrix\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        cm = confusion_matrix(y_test, y_pred, normalize='true')\n",
        "\n",
        "        cm_all[:, :, r] = cm\n",
        "        scores_all.append(acc)\n",
        "\n",
        "    # Mean and std\n",
        "    cm_mean = cm_all.mean(axis=2)\n",
        "    cm_std = cm_all.std(axis=2)\n",
        "    acc_mean = np.mean(scores_all)\n",
        "    acc_std = np.std(scores_all)\n",
        "\n",
        "    # Visualization\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm_mean)\n",
        "    disp.plot()\n",
        "    plt.title(f\"{model_name} | Acc: {acc_mean:.2f} Â± {acc_std:.2f}\")\n",
        "    plt.show()\n",
        "\n",
        "    return acc_mean, acc_std, cm_mean, cm_std"
      ],
      "metadata": {
        "id": "rz1yLW0dbQiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc_mean, acc_std, cm_mean, cm_std = run_ml_model_multiple(\n",
        "    X_all, y_all,\n",
        "    DecisionTreeClassifier,\n",
        "    \"Decision Tree\",\n",
        "    n_realizations=20,\n",
        "    max_depth=20\n",
        ")"
      ],
      "metadata": {
        "id": "QFao8fFPwmma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Neural network model"
      ],
      "metadata": {
        "id": "bNKQ20UlbPaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_nn(input_size, num_classes):\n",
        "    model = Sequential([\n",
        "        Dense(input_size, activation='relu', input_shape=(input_size,)),\n",
        "        Dense(2 * input_size, activation='relu'),\n",
        "        Dense(4 * input_size, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "oQWE0NwIbJoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_realizations = 20\n",
        "epochs = 50\n",
        "batch_size = 64\n",
        "num_classes = len(np.unique(y_all))\n",
        "input_size = X_all.shape[1]"
      ],
      "metadata": {
        "id": "VRtRA1hdvkog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss_all = np.zeros((epochs, n_realizations))\n",
        "val_loss_all = np.zeros((epochs, n_realizations))\n",
        "train_acc_all = np.zeros((epochs, n_realizations))\n",
        "val_acc_all = np.zeros((epochs, n_realizations))\n",
        "cm_all = np.zeros((num_classes, num_classes, n_realizations))\n",
        "scores_all = []"
      ],
      "metadata": {
        "id": "nunYoD2CvokG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for r in range(n_realizations):\n",
        "    print(f\"Realization {r+1}/{n_realizations}\")\n",
        "    X_train, X_val, X_test, y_train, y_val, y_test = get_splits(X_all, y_all)\n",
        "    y_train_cat = to_categorical(y_train, num_classes)\n",
        "    y_val_cat = to_categorical(y_val, num_classes)\n",
        "\n",
        "    model = build_nn(input_size, num_classes)\n",
        "    history = model.fit(\n",
        "        X_train, y_train_cat,\n",
        "        validation_data=(X_val, y_val_cat),\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "     # Learning rate\n",
        "    train_loss_all[:, r] = history.history['loss']\n",
        "    val_loss_all[:, r] = history.history['val_loss']\n",
        "    train_acc_all[:, r] = history.history['accuracy']\n",
        "    val_acc_all[:, r] = history.history['val_accuracy']\n",
        "\n",
        "    # Test set\n",
        "    y_pred = np.argmax(model.predict(X_test, verbose=0), axis=1)\n",
        "    score = accuracy_score(y_test, y_pred)\n",
        "    scores_all.append(score)\n",
        "\n",
        "    cm = confusion_matrix(y_test, y_pred, normalize='true')\n",
        "    cm_all[:, :, r] = cm"
      ],
      "metadata": {
        "id": "znHI69bEvrXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Calculate mean and std"
      ],
      "metadata": {
        "id": "SlQj73Fxv0ar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss_mean = train_loss_all.mean(axis=1)\n",
        "val_loss_mean = val_loss_all.mean(axis=1)\n",
        "train_acc_mean = train_acc_all.mean(axis=1)\n",
        "val_acc_mean = val_acc_all.mean(axis=1)\n",
        "\n",
        "train_loss_std = train_loss_all.std(axis=1)\n",
        "val_loss_std = val_loss_all.std(axis=1)\n",
        "train_acc_std = train_acc_all.std(axis=1)\n",
        "val_acc_std = val_acc_all.std(axis=1)\n",
        "\n",
        "cm_mean = cm_all.mean(axis=2)\n",
        "cm_std = cm_all.std(axis=2)\n",
        "score_avg = np.mean(scores_all)"
      ],
      "metadata": {
        "id": "FtY-vYvqvzhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Learning rate curve"
      ],
      "metadata": {
        "id": "0JKiAYkCbHB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_vec = np.arange(1, epochs+1)\n",
        "clrs = sns.color_palette(\"flare\")\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
        "# Accuracy\n",
        "ax[0].plot(epoch_vec, train_acc_mean, label='Train', color=clrs[0])\n",
        "ax[0].fill_between(epoch_vec, train_acc_mean-train_acc_std, train_acc_mean+train_acc_std, alpha=0.3)\n",
        "ax[0].plot(epoch_vec, val_acc_mean, label='Val', color=clrs[1])\n",
        "ax[0].fill_between(epoch_vec, val_acc_mean-val_acc_std, val_acc_mean+val_acc_std, alpha=0.3)\n",
        "ax[0].set_title(\"Accuracy\")\n",
        "ax[0].legend()\n",
        "\n",
        "# Loss\n",
        "ax[1].plot(epoch_vec, train_loss_mean, label='Train', color=clrs[0])\n",
        "ax[1].fill_between(epoch_vec, train_loss_mean-train_loss_std, train_loss_mean+train_loss_std, alpha=0.3)\n",
        "ax[1].plot(epoch_vec, val_loss_mean, label='Val', color=clrs[1])\n",
        "ax[1].fill_between(epoch_vec, val_loss_mean-val_loss_std, val_loss_mean+val_loss_std, alpha=0.3)\n",
        "ax[1].set_title(\"Loss\")\n",
        "ax[1].legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "puSVuZ0hbEce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11. Neural network confusion matrix"
      ],
      "metadata": {
        "id": "lGpUNnDebCNE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PabJ5wLEa24X"
      },
      "outputs": [],
      "source": [
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm_mean)\n",
        "disp.plot()\n",
        "plt.title(f\"Average Confusion Matrix | Accuracy: {score_avg:.2f}\")\n",
        "plt.show()"
      ]
    }
  ]
}