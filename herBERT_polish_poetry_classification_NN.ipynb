{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_sPMRqzjAB5"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install sacremoses"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "import joblib\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import HerbertTokenizer, RobertaModel, AutoTokenizer, BertModel\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE"
      ],
      "metadata": {
        "id": "vIL-B73HjKly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.mkdir(\"figs\")"
      ],
      "metadata": {
        "id": "kbQ4fBRLjLdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "herbert_klej = [\"Herbert-klej\",\n",
        "                HerbertTokenizer.from_pretrained(\"allegro/herbert-klej-cased-tokenizer-v1\"),\n",
        "                RobertaModel.from_pretrained(\"allegro/herbert-klej-cased-v1\")]"
      ],
      "metadata": {
        "id": "A_qrSO2tjhk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def common_compute(model, batch):\n",
        "    x, y = batch\n",
        "    logits = model(x)\n",
        "    loss = F.cross_entropy(logits, y)\n",
        "    return logits, loss, y\n",
        "\n",
        "def train_batch(model, optimizer, batch):\n",
        "    logits, loss, y = common_compute(model, batch)\n",
        "    _, predicted = torch.max(logits.data, -1)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    return loss, (predicted == y).sum().item()\n",
        "\n",
        "def validate_batch(model, batch):\n",
        "    logits, loss, y = common_compute(model, batch)\n",
        "    _, predicted = torch.max(logits.data, -1)\n",
        "    return loss, (predicted == y).sum().item()\n",
        "\n",
        "def test_batch(model, batch):\n",
        "    logits, loss, y = common_compute(model, batch)\n",
        "    _, predicted = torch.max(logits.data, -1)\n",
        "    return np.array(y).size, (predicted == y).sum().item(), loss, predicted, y"
      ],
      "metadata": {
        "id": "A5kGLaQ3jroZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# uśredniona macierz konfizji\n",
        "\n",
        "def get_confusion_matrix(CM_avrg, CM_std, model_name, data_type, classes, normalize):\n",
        "\n",
        "  cls = []\n",
        "  for k in classes.keys():\n",
        "    cls.append(k)\n",
        "\n",
        "  tick_marks = np.arange(len(cls))\n",
        "  cms = {\"Average\": CM_avrg, \"Std\": CM_std}\n",
        "\n",
        "\n",
        "  fig, axes = plt.subplots(1, 2, figsize=(20,10), sharey='row')\n",
        "\n",
        "  for i, (key, cm) in enumerate(cms.items()):\n",
        "\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=cls)\n",
        "    disp.plot(ax=axes[i], xticks_rotation=45)\n",
        "    disp.ax_.set_title(\"{} | Model: Neural Network | Data type: {} | Acc: {}\".format(key, data_type, round(score_avrg,2)))\n",
        "    disp.im_.colorbar.remove()\n",
        "    disp.ax_.set_xlabel('')\n",
        "    disp.ax_.set_ylabel('')\n",
        "\n",
        "\n",
        "  fig.text(0.40, 0.1, 'Predicted label', ha='left')\n",
        "  plt.subplots_adjust(wspace=0.40, hspace=0.1)\n",
        "\n",
        "  fig.colorbar(disp.im_, ax=axes)\n",
        "  plt.show()\n",
        "\n",
        "  plt.gcf().set_size_inches(10, 5)\n",
        "  fig.savefig('/content/figs/avrg_neural_network_{}_{}_{}.png'.format(model_name, data_type, normalize), dpi=200)\n",
        "  files.download('/content/figs/avrg_neural_network_{}_{}_{}.png'.format(model_name, data_type, normalize))"
      ],
      "metadata": {
        "id": "lSsgvOPNn0vz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# uśredniona krzywa uczenia\n",
        "\n",
        "def draw_learning_curve(model_name, history, data_type, n_epochs, normalize, key='accuracy'):\n",
        "\n",
        "  val_loss_mean, train_loss_mean, val_acc_mean, train_acc_mean, val_loss_std, train_loss_std, val_acc_std, train_acc_std = history.values()\n",
        "\n",
        "  fontsize = 16\n",
        "  epoch_vec = np.arange(0,n_epochs)\n",
        "  fig, ax = plt.subplots(1,2,figsize = (12, 8))\n",
        "  clrs = sns.color_palette(\"flare\")\n",
        "\n",
        "  #ax[1].set_ylim([0,2])\n",
        "  ax[1].plot(epoch_vec, train_loss_mean, label = \"train\")\n",
        "  ax[1].fill_between(epoch_vec, train_loss_mean - train_loss_std, train_loss_mean + train_loss_std, alpha = 0.3, facecolor=clrs[4] )\n",
        "  ax[1].plot(val_loss_mean,  label = \"val\")\n",
        "  ax[1].fill_between(epoch_vec, val_loss_mean - val_loss_std, val_loss_mean + val_loss_std, alpha = 0.3, facecolor=clrs[4] )\n",
        "\n",
        "  ax[0].plot(epoch_vec, train_acc_mean,  label = \"train\")\n",
        "  ax[0].fill_between(epoch_vec, train_acc_mean - train_acc_std, train_acc_mean + train_acc_std, alpha = 0.3, facecolor=clrs[4])\n",
        "  ax[0].plot(val_acc_mean,  label = \"val\")\n",
        "  ax[0].fill_between(epoch_vec, val_acc_mean - val_acc_std, val_acc_mean + val_acc_std, alpha = 0.3, facecolor=clrs[4] )\n",
        "\n",
        "\n",
        "  ax[1].set_xlabel(\"Traning epoch\", fontsize=fontsize)\n",
        "  ax[1].set_ylabel(\"Loss\", fontsize=fontsize)\n",
        "  ax[1].set_yscale('log')\n",
        "  ax[0].set_xlabel(\"Traning epoch\", fontsize=fontsize)\n",
        "  ax[0].set_ylabel(\"Accuracy\", fontsize=fontsize)\n",
        "  ax[0].set_yscale('log')\n",
        "\n",
        "  ax[1].legend( fontsize = fontsize)\n",
        "  ax[0].legend( fontsize = fontsize)\n",
        "  fig.suptitle('Learning curve | Data type: {}'.format(data_type))\n",
        "\n",
        "  fig.savefig('/content/figs/avgr_learning_curve_{}_{}_{}.png'.format(model_name, data_type, normalize))\n",
        "  files.download('/content/figs/avgr_learning_curve_{}_{}_{}.png'.format(model_name, data_type, normalize))"
      ],
      "metadata": {
        "id": "-bfhWw0JibrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_classes(df):\n",
        "  authors = {}\n",
        "  y = df['Label']\n",
        "  if len(df['Label'].unique()) < 8:\n",
        "    y = df ['Label'].factorize()[0]\n",
        "  num_classes = len(df['Label'].unique())\n",
        "  for label in range(0, num_classes):\n",
        "    i, = np.where(y == label)\n",
        "    authors['{}'.format(df['Author-short'][i[0]])] = label\n",
        "\n",
        "  return authors"
      ],
      "metadata": {
        "id": "zt0V-Lyvj2JS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_embedding(df, model):\n",
        "\n",
        "  X_stack = []\n",
        "  model_name, tokenizer, model = model\n",
        "  embedded = {}\n",
        "  tokens = {}\n",
        "  num_idxs = df.shape[0]\n",
        "  for idx in tqdm(range(0,num_idxs)):\n",
        "    single_poem_input = df['Text'][idx]\n",
        "    inputs = tokenizer.batch_encode_plus([single_poem_input], max_length = 512, padding=\"longest\", add_special_tokens=True, return_tensors=\"pt\",)\n",
        "    single_poem_output = model(**inputs)\n",
        "    X_single_poem = single_poem_output[0][:,0,:].detach().numpy()\n",
        "    X_stack.append(X_single_poem[0])\n",
        "\n",
        "    embedded[idx] = X_single_poem[0], df['Label'][idx]\n",
        "\n",
        "  df_embedded = pd.DataFrame.from_dict(embedded,  orient='index', columns=['embedding', 'label'])\n",
        "\n",
        "  return df_embedded"
      ],
      "metadata": {
        "id": "RbHAswlWj4Uo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_data(X):\n",
        "\n",
        "  X_normalized = np.zeros((X.shape[0],X.shape[1]))\n",
        "\n",
        "  for idx in range(0,X.shape[0]):\n",
        "    X_normalized[idx,:] = (X[idx,:] - np.mean(X[idx,:]))/ np.std(X[idx,:])\n",
        "\n",
        "  return X_normalized"
      ],
      "metadata": {
        "id": "YTfD6GWQj6YZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_experiment(embedding_model, df, data_type, n_realizations, normalization):\n",
        "\n",
        "  normalize = \"normalized.0\"\n",
        "  if(normalization==True):\n",
        "    normalize = \"normalized.1\"\n",
        "  model_name = embedding_model[0]\n",
        "\n",
        "  n_classes = len(df['Label'].unique())\n",
        "  CM = np.zeros((n_classes, n_classes, n_realizations))\n",
        "  scores = []\n",
        "  n_epochs = 20\n",
        "\n",
        "  train_loss_realizations = np.zeros((n_epochs, n_realizations))\n",
        "  train_acc_realizations = np.zeros((n_epochs, n_realizations))\n",
        "  val_loss_realizations = np.zeros((n_epochs, n_realizations))\n",
        "  val_acc_realizations = np.zeros((n_epochs, n_realizations))\n",
        "\n",
        "  embed_data = make_embedding(df, embedding_model)\n",
        "  df = pd.concat([df, embed_data['embedding']], axis=1)\n",
        "  for n in range(0, n_realizations):\n",
        "    # podział danych na dane testowe oraz treningowe i validacyjne w celu przeprowadzenia n niezależnych realizacji\n",
        "\n",
        "    df_train, df_test, _, _ = train_test_split(df, test_size=0.2)\n",
        "    df_train, df_val, _, _ = train_test_split(df_train, test_size=0.1)\n",
        "\n",
        "    X_train = np.stack(df_train['embedding'])\n",
        "    y_train = df_train['Label'].values\n",
        "\n",
        "    X_val = np.stack(df_val['embedding'])\n",
        "    y_val = df_val['Label'].values\n",
        "\n",
        "    X_test = np.stack(df_test['embedding'])\n",
        "    y_test = df_test['Label'].values\n",
        "\n",
        "    train_dataset = TensorDataset(torch.FloatTensor(X_train),torch.LongTensor(y_train))\n",
        "    val_dataset = TensorDataset(torch.FloatTensor(X_val),torch.LongTensor(y_val))\n",
        "    test_dataset = TensorDataset(torch.FloatTensor(X_test),torch.LongTensor(y_test))\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=1024,shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=1024,shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1024,shuffle=True)\n",
        "\n",
        "    model_NN = MLPClassifier()\n",
        "    optimizer = torch.optim.Adam(model_NN.parameters(), lr = 1e-4)\n",
        "\n",
        "    # pętla trenowania\n",
        "    n_epochs = n_epochs\n",
        "    for epoch in range(n_epochs):\n",
        "      model_NN.train()\n",
        "      train_loss = []\n",
        "      train_acc = []\n",
        "      bar = tqdm(train_loader, position=0, leave=False, desc='epoch %d'%epoch)\n",
        "      for batch in bar:\n",
        "        loss, acc = train_batch(model_NN, optimizer, batch)\n",
        "        train_loss.append(loss)\n",
        "        train_acc.append(acc)\n",
        "        avg_train_loss = torch.stack(train_loss).mean()\n",
        "        avg_train_loss = avg_train_loss.detach().numpy()\n",
        "        avg_train_acc = np.stack(train_acc).mean()\n",
        "      print('train_loss', avg_train_loss.item())\n",
        "      train_loss_realizations[epoch,n] = avg_train_loss\n",
        "      train_acc_realizations[epoch,n] = avg_train_acc\n",
        "\n",
        "      model_NN.eval()\n",
        "      with torch.no_grad():\n",
        "        val_loss = []\n",
        "        val_acc = []\n",
        "        for batch in val_loader:\n",
        "          loss, acc = validate_batch(model_NN, batch)\n",
        "          val_loss.append(loss)\n",
        "          val_acc.append(acc)\n",
        "          avg_val_loss = torch.stack(val_loss).mean()\n",
        "          avg_val_loss = avg_val_loss.detach().numpy()\n",
        "          avg_val_acc = np.stack(val_acc).mean()\n",
        "        print('val_loss', avg_val_loss.item())\n",
        "\n",
        "        val_loss_realizations[epoch,n] = avg_val_loss\n",
        "        val_acc_realizations[epoch,n] = avg_val_acc\n",
        "\n",
        "    classes = print_classes(df_test)\n",
        "\n",
        "    # pętla testowania\n",
        "    bar = tqdm(test_loader, position=0, leave=False, desc='test')\n",
        "    test_loss = []\n",
        "    true_labels = []\n",
        "    pred_labels = []\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in bar:\n",
        "            batch_size, batch_correct, loss, predicted, y = test_batch(model_NN, batch)\n",
        "            total += batch_size\n",
        "            correct += batch_correct\n",
        "            test_loss.append(loss)\n",
        "            true_labels.append(predicted)\n",
        "            pred_labels.append(y)\n",
        "        print('Acc: {}'.format(100 * float(correct) / total))\n",
        "        score = (100 * float(correct) / total)\n",
        "        cm = confusion_matrix(true_labels, pred_labels, normalize='true')\n",
        "        CM[:,:,n] = cm\n",
        "        scores.append(score)\n",
        "\n",
        "# dane do uśrednionej krzywej uczenia\n",
        "  val_loss_mean = np.mean(val_loss_realizations, axis=1)\n",
        "  train_loss_mean = np.mean(train_loss_realizations, axis=1)\n",
        "  val_acc_mean = np.mean(val_acc_realizations, axis=1)\n",
        "  train_acc_mean = np.mean(train_acc_realizations, axis=1)\n",
        "\n",
        "  val_loss_std = np.std(val_loss_realizations, axis=1)\n",
        "  train_loss_std = np.std(train_loss_realizations, axis=1)\n",
        "  val_acc_std = np.std(val_acc_realizations, axis=1)\n",
        "  train_acc_std = np.std(train_acc_realizations, axis=1)\n",
        "\n",
        "  dict_history = {\"val_loss_mean\":val_loss_mean,\n",
        "                  \"train_loss_mean\":train_loss_mean,\n",
        "                  \"val_acc_mean\":val_acc_mean,\n",
        "                  \"train_acc_mean\":train_acc_mean,\n",
        "                  \"val_loss_std\":val_loss_std,\n",
        "                  \"train_loss_std\":train_loss_std,\n",
        "                  \"val_acc_std\":val_acc_std,\n",
        "                  \"train_acc_std\":train_acc_std}\n",
        "\n",
        "# uśredniona krzywa uczenia\n",
        "  draw_learning_curve(model_name, dict_history, data_type, n_epochs, normalize, key='accuracy')\n",
        "\n",
        "# uśredniona macierz konfuzji\n",
        "  CM_avrg = np.zeros((n_classes,n_classes))\n",
        "  CM_std = np.zeros((n_classes,n_classes))\n",
        "  score_avrg = np.mean(scores)\n",
        "\n",
        "  for i in range(0,n_classes):\n",
        "    for j in range(0,n_classes):\n",
        "      CM_avrg[i,j] = np.mean(CM[i,j,:])\n",
        "      CM_std[i,j] = np.std(CM[i,j,:])\n",
        "\n",
        "  classes = print_classes(df)\n",
        "\n",
        "  get_confusion_matrix(CM_avrg, CM_std, model_name, data_type, classes, normalize)\n",
        "\n",
        "  return score_avrg, dict_history"
      ],
      "metadata": {
        "id": "pS5LEAzakDi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "      self.fc1 = nn.Linear(768, 768*2)\n",
        "      self.fc2 = nn.Linear(768*2, 768*4)\n",
        "      self.out = nn.Linear(768*4, 4)\n",
        "      self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = x.view(x.size(0))\n",
        "      x = F.relu(self.fc1(x))\n",
        "      x = F.relu(self.fc2(x))\n",
        "      x = self.dropout(x)\n",
        "      x = self.out(x)\n",
        "      return x"
      ],
      "metadata": {
        "id": "lajLqMomnkiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_raw = pd.read_csv('/content/polish_poetry.csv', \";\")\n",
        "df_raw.shape"
      ],
      "metadata": {
        "id": "wr3uN3Kgol_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_women = pd.concat([df_raw[\"Text\"],df_raw[\"Label\"],df_raw[\"Author-short\"]], axis=1)\n",
        "df_women = df_women[200:].reset_index(drop=True)\n",
        "df_men = pd.concat([df_raw[\"Text\"],df_raw[\"Label\"],df_raw[\"Author-short\"]], axis=1)\n",
        "df_men = df_men[:200].reset_index(drop=True)"
      ],
      "metadata": {
        "id": "W-oxOrT9owKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all = pd.DataFrame\n",
        "df_all = pd.concat([df_raw[\"Text\"],df_raw[\"Label\"],df_raw[\"Author-short\"]], axis=1)\n",
        "df_all = df_all.sample(frac = 1).reset_index(drop=True)\n",
        "df_all"
      ],
      "metadata": {
        "id": "9MM4Rx6SoyPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df_all\n",
        "model = herbert_klej\n",
        "data_type = 'all'\n",
        "n_realization = 10\n",
        "\n",
        "score, dict_history_men = make_experiment(model, df, data_type, n_realizations, normalization=True)"
      ],
      "metadata": {
        "id": "dF5FcDkAo1LK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}