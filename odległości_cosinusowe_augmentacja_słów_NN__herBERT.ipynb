{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_17sa8j9Cg4"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install sacremoses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLM5dKam9Vo6"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "import joblib\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from transformers import HerbertTokenizer, RobertaModel, AutoTokenizer, BertModel\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Y302eXuWka5"
      },
      "outputs": [],
      "source": [
        "os.mkdir(\"figs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtLD7hgHY8Kt"
      },
      "outputs": [],
      "source": [
        "def get_confusion_matrix(model, X_test, y_test, model_name, data_type, classes):\n",
        "\n",
        "  cls =[]\n",
        "  for k in classes.keys():\n",
        "    cls.append(k)\n",
        "\n",
        "  y_pred = model.predict(X_test)\n",
        "  pred_labels=[]\n",
        "  for idx in range(len(y_pred)):\n",
        "    pred_label = np.argmax(y_pred[idx])\n",
        "    pred_labels.append(pred_label)\n",
        "\n",
        "  true_labels = y_test\n",
        "  score = accuracy_score(true_labels, pred_labels)\n",
        "  cm = confusion_matrix(true_labels, pred_labels, normalize='true')\n",
        "\n",
        "  #disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "  #disp.plot()\n",
        "  #disp.ax_.set_title(\"Model: {} | Data type: {} |  Acc: {}\".format(model_name, data_type, score))\n",
        "\n",
        "  #if classes is not None:\n",
        "      #tick_marks = np.arange(len(cls))\n",
        "      #plt.xticks(tick_marks, cls, rotation=45)\n",
        "      #plt.yticks(tick_marks, cls, rotation=50)\n",
        "\n",
        "  #plt.gcf().set_size_inches(10, 10)\n",
        "  #plt.savefig('/content/figs/{}_{}.png'.format(model_name, data_type), dpi=200)\n",
        "  #files.download('/content/figs/{}_{}.png'.format(model_name, data_type))\n",
        "\n",
        "  return score, cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UkHm_7f9coo"
      },
      "outputs": [],
      "source": [
        "def draw_learning_curve(history, data_type, key='accuracy'):\n",
        "\n",
        "  fig, ax = plt.subplots(1, 2, figsize=(12,6))\n",
        "  ax[0].plot(history.history[key])\n",
        "  ax[0].plot(history.history['val_'+ key] )\n",
        "  ax[0].set_ylabel(key.title())\n",
        "  ax[0].set_xlabel('Epoch')\n",
        "  ax[0].legend(['train', 'val'])\n",
        "\n",
        "  ax[1].plot(history.history['loss'])\n",
        "  ax[1].plot(history.history['val_loss'] )\n",
        " # ax[1].set_ylim([0,1])\n",
        "  ax[1].set_ylabel('loss'.title())\n",
        "  ax[1].set_xlabel('Epoch')\n",
        "  ax[1].legend(['train', 'val'])\n",
        "  fig.suptitle('Learning curve | Data type: {}'.format(data_type))\n",
        "  plt.show()\n",
        "  #fig.savefig('/content/figs/learning_curve_{}.png'.format(data_type))\n",
        "  #files.download('/content/figs/learning_curve_{}.png'.format(data_type))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYBaBrbT9xTN"
      },
      "outputs": [],
      "source": [
        "def get_data_set(labels, df):\n",
        "  idxs = []\n",
        "  for label in labels:\n",
        "    idxs_for_label, = np.where(df['Label'] == label)\n",
        "    for idx in idxs_for_label:\n",
        "      idxs.append(idx)\n",
        "\n",
        "  new_df = df.iloc[idxs]\n",
        "  new_df = new_df.sample(frac = 1).reset_index(drop=True)\n",
        "  return new_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7MoiQjw-x5D"
      },
      "outputs": [],
      "source": [
        "def print_classes(df):\n",
        "  authors = {}\n",
        "  y = df['Label']\n",
        "  if len(df['Label'].unique()) < 8:\n",
        "    y = df ['Label'].factorize()[0]\n",
        "  num_classes = len(df['Label'].unique())\n",
        "  for label in range(0, num_classes):\n",
        "    i, = np.where(y == label)\n",
        "    authors['{}'.format(df['Author-short'][i[0]])] = label\n",
        "\n",
        "  return authors"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_embedding(df, model):\n",
        "\n",
        "  X_stack = []\n",
        "  model_name, tokenizer, model = model\n",
        "  embedded = {}\n",
        "  tokens = {}\n",
        "  num_idxs = df.shape[0]\n",
        "  for idx in tqdm(range(0,num_idxs)):\n",
        "    single_poem_input = df['Text'][idx]\n",
        "    inputs = tokenizer.batch_encode_plus([single_poem_input], max_length = 512, padding=\"longest\", add_special_tokens=True, return_tensors=\"pt\",)\n",
        "    single_poem_output = model(**inputs)\n",
        "    X_single_poem = single_poem_output[0][:,0,:].detach().numpy()\n",
        "    X_stack.append(X_single_poem[0])\n",
        "\n",
        "    embedded[idx] = X_single_poem[0], df['Label'][idx]\n",
        "\n",
        "  df_embedded = pd.DataFrame.from_dict(embedded,  orient='index', columns=['embedding', 'label'])\n",
        "\n",
        "  return df_embedded"
      ],
      "metadata": {
        "id": "24rzVy9CQTxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_data(X):\n",
        "\n",
        "  X_normalized = np.zeros((X.shape[0],X.shape[1]))\n",
        "\n",
        "  for idx in range(0,X.shape[0]):\n",
        "    X_normalized[idx,:] = (X[idx,:] - np.mean(X[idx,:]))/ np.std(X[idx,:])\n",
        "\n",
        "  return X_normalized"
      ],
      "metadata": {
        "id": "xls271BrQf1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_X_y(df, normalization=True):\n",
        "\n",
        "  X = np.stack(df['embedding'])\n",
        "  y = df['Label']\n",
        "  if(normalization==True):\n",
        "    X = normalize_data(X)\n",
        "\n",
        "  #jeśli jest mniej niż 8 klas:\n",
        "  if len(df['Label'].unique()) < 8:\n",
        "    y = df ['Label'].factorize()[0]\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "  X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
        "\n",
        "  print(X.shape)\n",
        "\n",
        "  return X, y, X_train, X_val, X_test, y_train, y_val, y_test"
      ],
      "metadata": {
        "id": "KGHRxlCPYnV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_X_y_train(df, normalization=True):\n",
        "\n",
        "  X = np.stack(df['embedding'])\n",
        "  y = df['Label'].values\n",
        "  if(normalization==True):\n",
        "    X = normalize_data(X)\n",
        "\n",
        "  #jeśli jest mniej niż 8 klas:\n",
        "  if len(df['Label'].unique()) < 8:\n",
        "    y = df ['Label'].factorize()[0]\n",
        "\n",
        "  X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "\n",
        "  print(X.shape)\n",
        "\n",
        "  return X, y, X_train, X_val, y_train, y_val"
      ],
      "metadata": {
        "id": "c_XQaNLoQh2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cosinus_predictions(X_train, y_train, X_test, y_test):\n",
        "\n",
        " # X_train_normalized = get_normalization(X_train)\n",
        " # X_test_normalized  = get_normalization(X_test)\n",
        "\n",
        "  train_labels = np.unique(y_train)\n",
        "  test_labels  = np.unique(y_test)\n",
        "  confusion_matrix = np.zeros((train_labels.shape[0], test_labels.shape[0]))\n",
        "\n",
        "  for idx_x_test, x_test in enumerate(X_test):\n",
        "    y_true = y_test[idx_x_test]\n",
        "\n",
        "    cos_distance_min = 10000\n",
        "    y_pred = 0\n",
        "    for idx_x_train, x_train in enumerate(X_train):\n",
        "      cos_distance = np.dot(x_test,x_train)\n",
        "      if(cos_distance < cos_distance_min):\n",
        "        cos_distance_min = cos_distance\n",
        "        y_pred = y_train[idx_x_train]\n",
        "\n",
        "    confusion_matrix[y_true, y_pred] = confusion_matrix[y_pred, y_true] + 1\n",
        "\n",
        "  for label in test_labels:\n",
        "    n_y_true = np.where(label == y_test)[0]\n",
        "    confusion_matrix[y_true, :] = confusion_matrix[y_true, :]/n_y_true*100\n",
        "\n",
        "  plt.imshow(confusion_matrix)\n",
        "  plt.colorbar()\n",
        "  return confusion_matrix\n"
      ],
      "metadata": {
        "id": "mTe67zPje9M0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_orginal = pd.DataFrame\n",
        "df_orginal = pd.concat([df_raw[\"Text\"],df_raw[\"Label\"],df_raw[\"Author-short\"]], axis=1)\n",
        "#df_orginal = df_orginal.sample(frac = 1).reset_index(drop=True)\n",
        "df_orginal"
      ],
      "metadata": {
        "id": "A3QHV80-RZLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed = make_embedding(df_orginal, herbert_klej)\n",
        "#embed = make_embedding(df_orginal, herbert_large)\n",
        "df_orginal = pd.concat([df_orginal, embed['embedding']], axis=1)\n",
        "df_orginal"
      ],
      "metadata": {
        "id": "X1UhyMSml8OA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_men = make_embedding(df_men, herbert_klej)\n",
        "df_men = pd.concat([df_men, embed_men['embedding']], axis=1)\n",
        "df_men"
      ],
      "metadata": {
        "id": "030Ec2CwcHb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_women = make_embedding(df_women, herbert_klej)\n",
        "df_women = pd.concat([df_women, embed_women['embedding']], axis=1)\n",
        "df_women"
      ],
      "metadata": {
        "id": "82BfcwmKcQPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df_women\n",
        "norm = False\n",
        "data_type = 'women'\n",
        "\n",
        "classes = print_classes(df)\n",
        "cls =[]\n",
        "for k in classes.keys():\n",
        "  cls.append(k)"
      ],
      "metadata": {
        "id": "Vj4on-6nn55v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_realizations = 10\n",
        "CM_aver = np.zeros((4,4))\n",
        "if norm==True:\n",
        "  normalize = 1\n",
        "else:\n",
        "  normalize = 0\n",
        "\n",
        "for n_realization in range(n_realizations):\n",
        "  X, y, X_train, X_test, y_train, y_test = get_X_y_train(df, normalization=False)\n",
        "\n",
        "  confusion_matrix = np.zeros((4,4))\n",
        "  train_labels = np.unique(y_train)\n",
        "  test_labels  = np.unique(y_test)\n",
        "\n",
        "  for idx_x_test in range(0,X_test.shape[0]):\n",
        "\n",
        "    x_test = X_test[idx_x_test,:]\n",
        "    y_true = int(y_test[idx_x_test])\n",
        "\n",
        "    distance_min = 10000\n",
        "    y_pred = 0\n",
        "\n",
        "    distance_from_x_train = np.zeros(X_train.shape[0])\n",
        "    for idx_x_train in range(X_train.shape[0]):\n",
        "      x_train = X_train[idx_x_train, :]\n",
        "      #distance = np.dot(x_test, x_train)\n",
        "      distance = np.sqrt(np.sum((x_train-x_test)**2)) #euclidean distance\n",
        "      distance_from_x_train[idx_x_train] = distance\n",
        "\n",
        "    idx_min_distance = np.argmin(distance_from_x_train)\n",
        "    y_pred = int(y_train[idx_min_distance])\n",
        "    #print(y_pred)\n",
        "      #print(y_true, y_pred)\n",
        "\n",
        "    confusion_matrix[y_true, y_pred] = confusion_matrix[ y_true, y_pred] + 1\n",
        "\n",
        "  for y_true in test_labels:\n",
        "\n",
        "    y_true = int(y_true)\n",
        "    confusion_matrix[y_true, :] = confusion_matrix[ y_true, :]/np.sum(confusion_matrix[y_true,:])\n",
        "\n",
        "  #print(n_realization, np.mean(np.diag(confusion_matrix)))\n",
        "  CM_aver = CM_aver + confusion_matrix\n",
        "CM_aver = CM_aver/n_realizations\n",
        "acc =  round(np.mean(np.diag(CM_aver)), 2)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=CM_aver)\n",
        "disp.plot()\n",
        "disp.ax_.set_title(\"Prediction by euclidean distance | Data type: {} |  Acc: {}\".format(data_type, acc))\n",
        "tick_marks = np.arange(len(cls))\n",
        "plt.xticks(tick_marks, cls, rotation=45)\n",
        "plt.yticks(tick_marks, cls, rotation=50)\n",
        "\n",
        "plt.gcf().set_size_inches(10, 10)\n",
        "plt.savefig('/content/figs/prediction_euclidean_distance_{}_normalize_{}.png'.format(data_type, normalize), dpi=200)\n",
        "#files.download('/content/figs/prediction_euclidean_distance_{}_normalize_{}.png'.format(data_type, normalize))\n",
        "\n",
        "\n",
        "\n",
        "print(np.mean(np.diag(CM_aver)))"
      ],
      "metadata": {
        "id": "xmZvmT8TchkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Distance from averaged vectors\n",
        "\n",
        "n_realizations = 100\n",
        "CM_aver = np.zeros((8,8))\n",
        "for n_realization in range(n_realizations):\n",
        "  X, y, X_train, X_test, y_train, y_test = get_X_y_train(df, normalization=True)\n",
        "\n",
        "  idx_class_0 = np.where(y_train == 0)[0]\n",
        "  idx_class_1 = np.where(y_train == 1)[0]\n",
        "  idx_class_2 = np.where(y_train == 2)[0]\n",
        "  idx_class_3 = np.where(y_train == 3)[0]\n",
        "  idx_class_4 = np.where(y_train == 4)[0]\n",
        "  idx_class_5 = np.where(y_train == 5)[0]\n",
        "  idx_class_6 = np.where(y_train == 6)[0]\n",
        "  idx_class_7 = np.where(y_train == 7)[0]\n",
        "\n",
        "  X_aver = np.zeros((8,X_train.shape[1]))\n",
        "\n",
        "  X_aver[0,:] = np.mean(X_train[idx_class_0,:],axis=0)\n",
        "  X_aver[1,:] = np.mean(X_train[idx_class_1,:],axis=0)\n",
        "  X_aver[2,:] = np.mean(X_train[idx_class_2,:],axis=0)\n",
        "  X_aver[3,:] = np.mean(X_train[idx_class_3,:],axis=0)\n",
        "  X_aver[4,:] = np.mean(X_train[idx_class_4,:],axis=0)\n",
        "  X_aver[5,:] = np.mean(X_train[idx_class_5,:],axis=0)\n",
        "  X_aver[6,:] = np.mean(X_train[idx_class_6,:],axis=0)\n",
        "  X_aver[7,:] = np.mean(X_train[idx_class_7,:],axis=0)\n",
        "\n",
        "\n",
        "  confusion_matrix = np.zeros((  8, 8))\n",
        "  train_labels = np.unique(y_train)\n",
        "  test_labels  = np.unique(y_test)\n",
        "\n",
        "  for idx_x_test in range(0,X_test.shape[0]):\n",
        "\n",
        "    x_test = X_test[idx_x_test,:]\n",
        "    y_true = int(y_test[idx_x_test])\n",
        "\n",
        "    distance_min = 10000\n",
        "    y_pred = 0\n",
        "\n",
        "    distance_from_x_train = np.zeros(X_aver.shape[0])\n",
        "    for idx_x_train in range(X_aver.shape[0]):\n",
        "      x_aver_train = X_aver[idx_x_train, :]\n",
        "      #distance = np.dot(x_test, x_train)\n",
        "      distance = np.sqrt(np.sum((x_aver_train-x_test)**2)) #euclidean distance\n",
        "      distance_from_x_train[idx_x_train] = distance\n",
        "\n",
        "    idx_min_distance = np.argmin(distance_from_x_train)\n",
        "    y_pred = int(y_train[idx_min_distance])\n",
        "    #print(y_pred)\n",
        "      #print(y_true, y_pred)\n",
        "\n",
        "    confusion_matrix[y_true, y_pred] = confusion_matrix[ y_true, y_pred] + 1\n",
        "\n",
        "  for y_true in test_labels:\n",
        "\n",
        "    y_true = int(y_true)\n",
        "    confusion_matrix[y_true, :] = confusion_matrix[ y_true, :]/np.sum(confusion_matrix[y_true,:])\n",
        "\n",
        "  print(n_realization, np.mean(np.diag(confusion_matrix)))\n",
        "  CM_aver = CM_aver + confusion_matrix\n",
        "CM_aver = CM_aver/n_realizations\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=CM_aver)\n",
        "disp.plot()\n",
        "\n",
        "tick_marks = np.arange(len(cls))\n",
        "plt.xticks(tick_marks, cls, rotation=45)\n",
        "plt.yticks(tick_marks, cls, rotation=50)\n",
        "plt.gcf().set_size_inches(10, 10)\n",
        "\n",
        "\n",
        "\n",
        "print(np.mean(np.diag(CM_aver)))"
      ],
      "metadata": {
        "id": "fiAjrosGZo6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qKlYdQwvm1qW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_X_y_test(df, normalization=True):\n",
        "  X = np.stack(df['embedding'])\n",
        "  y = df['Label']\n",
        "  if(normalization == True):\n",
        "    X = normalize_data(X)\n",
        "\n",
        "  #jeśli jest mniej niż 8 klas:\n",
        "  if len(df['Label'].unique()) < 8:\n",
        "    y = df ['Label'].factorize()[0]\n",
        "\n",
        "  return X, y"
      ],
      "metadata": {
        "id": "BcvYHaP1QmLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    no_punct=[words for words in text if words not in string.punctuation]\n",
        "    words_wo_punct=''.join(no_punct)\n",
        "    return words_wo_punct"
      ],
      "metadata": {
        "id": "UcTyaa5T5UTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def swap_words(df):\n",
        "\n",
        "  dict_new_text = {}\n",
        "\n",
        "  for idx in tqdm(range(0,len(df))):\n",
        "    text_split = df['Text'][idx].split()\n",
        "    to_swap_list = []\n",
        "    #n_pairs = int((len(text_split) * fraction)/2)\n",
        "   # if(n_pairs==0):\n",
        "    n_pairs = 1\n",
        "\n",
        "    for i in range(0,n_pairs):\n",
        "      idx_to_swap_1 = random.randrange(0, len(text_split))\n",
        "      idx_to_swap_2 = random.randrange(0, len(text_split))\n",
        "      if(idx_to_swap_1 != idx_to_swap_2):\n",
        "        word_to_swap_1 = text_split[idx_to_swap_1]\n",
        "        word_to_swap_2 = text_split[idx_to_swap_2]\n",
        "        text_split[idx_to_swap_1] = word_to_swap_2\n",
        "        text_split[idx_to_swap_2] = word_to_swap_1\n",
        "\n",
        "    new_text = ' '.join(text_split)\n",
        "\n",
        "    dict_new_text[idx] = new_text, df['Label'][idx], df['Author-short'][idx]\n",
        "\n",
        "  df_swap_words = pd.DataFrame.from_dict(dict_new_text, orient='index', columns=['Text', 'Label', 'Author-short'])\n",
        "  return df_swap_words\n",
        "\n",
        "# dzielenie każdego wiersza na fragmenty o długości 60 słów\n",
        "def cut_with_window(df, window_size, step_size):\n",
        "\n",
        "  dict_new_text = {}\n",
        "\n",
        "  for idx in tqdm(range(0,len(df))):\n",
        "    text_split = df['Text'][idx].split()\n",
        "    x = 0\n",
        "    y = window_size\n",
        "    i = 0\n",
        "    while x < len(text_split):\n",
        "      #y = x + window_size\n",
        "      new_text_arr = text_split[x:y]\n",
        "      new_text = ' '.join(new_text_arr)\n",
        "\n",
        "      i = i + 1\n",
        "      x = x + step_size\n",
        "      y = min(x + window_size, len(text_split)-1)\n",
        "      dict_new_text[idx, i] = new_text, df['Label'][idx], df['Author-short'][idx]\n",
        "\n",
        "  df_cut = pd.DataFrame.from_dict(dict_new_text,  orient='index', columns=['Text', 'Label', 'Author-short'])\n",
        "  return df_cut\n",
        "\n",
        "# usuwanie kielku randomowo wybranych słów z orginalnego wiersza\n",
        "def remove_words(df, fraction):\n",
        "\n",
        "  dict_new_text = {}\n",
        "\n",
        "  for idx in tqdm(range(0,len(df))):\n",
        "    text_split = df['Text'][idx].split()\n",
        "    f = int(fraction * len(text_split))\n",
        "    if(f==0):\n",
        "      f = 1\n",
        "    to_remove_list = []\n",
        "\n",
        "    for i in range(0,f):\n",
        "      to_remove = random.randrange(0, len(text_split))\n",
        "      to_remove_list.append(to_remove)\n",
        "\n",
        "    for word_idx in to_remove_list:\n",
        "      text_split[word_idx] = ' '\n",
        "\n",
        "    new_text = ' '.join(text_split)\n",
        "\n",
        "    dict_new_text[idx] = new_text, df['Label'][idx], df['Author-short'][idx]\n",
        "\n",
        "  df_remove = pd.DataFrame.from_dict(dict_new_text,  orient='index', columns=['Text', 'Label', 'Author-short'])\n",
        "  return df_remove\n",
        "\n",
        "\n",
        "# podział wiersza na pół\n",
        "def cut_in_half(df):\n",
        "\n",
        "  new_dict_1 = {}\n",
        "  new_dict_2 = {}\n",
        "\n",
        "  for idx in tqdm(range(0,len(df))):\n",
        "\n",
        "    text_split = df['Text'][idx].split()\n",
        "    x = int(len(text_split)/2)\n",
        "    new_text_arr_1 = text_split[:x]\n",
        "    new_text_arr_2 = text_split[x:]\n",
        "    new_text_1 = \" \".join(new_text_arr_1)\n",
        "    new_text_2 = \" \".join(new_text_arr_2)\n",
        "\n",
        "    new_dict_1[idx] = new_text_1, df['Label'][idx], df['Author-short'][idx]\n",
        "    new_dict_2[idx] = new_text_2, df['Label'][idx], df['Author-short'][idx]\n",
        "\n",
        "  df_new_1 = pd.DataFrame.from_dict(new_dict_1,  orient='index', columns=['Text', 'Label', 'Author-short'])\n",
        "  df_new_2 = pd.DataFrame.from_dict(new_dict_2,  orient='index', columns=['Text', 'Label', 'Author-short'])\n",
        "\n",
        "  result = pd.concat([df_new_1, df_new_2])\n",
        "  df_cut = result.reset_index(drop=True)\n",
        "  df_cut\n",
        "  return df_cut\n",
        "\n",
        "#odbicie lustrzane wiersza\n",
        "def flip(df):\n",
        "  new_dict = {}\n",
        "\n",
        "  for idx in tqdm(range(0,len(df))):\n",
        "    text_split = df['Text'][idx].split()\n",
        "    text_split.reverse()\n",
        "    new_text = \" \".join(text_split)\n",
        "\n",
        "    new_dict[idx] = new_text, df['Label'][idx], df['Author-short'][idx]\n",
        "\n",
        "  df_flip = pd.DataFrame.from_dict(new_dict,  orient='index', columns=['Text', 'Label', 'Author-short'])\n",
        "  return df_flip\n",
        "\n",
        "def cut_and_flip(df, part):\n",
        "  new_dict = {}\n",
        "\n",
        "  for idx in tqdm(range(0,len(df))):\n",
        "    text_split = df['Text'][idx].split()\n",
        "    x = int(len(text_split)/2)\n",
        "    new_text_arr_1 = text_split[:x]\n",
        "    new_text_arr_2 = text_split[x:]\n",
        "    if(part==1):\n",
        "      new_text_arr_1.reverse()\n",
        "    if(part==2):\n",
        "      new_text_arr_2.reverse()\n",
        "    if(part==3):\n",
        "      new_text_arr_1.reverse()\n",
        "      new_text_arr_2.reverse()\n",
        "    new_text_arr = new_text_arr_1 + new_text_arr_2\n",
        "    new_text = \" \".join(new_text_arr)\n",
        "\n",
        "    new_dict[idx] = new_text, df['Label'][idx], df['Author-short'][idx]\n",
        "\n",
        "  df_cut_and_flip = pd.DataFrame.from_dict(new_dict,  orient='index', columns=['Text', 'Label', 'Author-short'])\n",
        "  return df_cut_and_flip\n",
        "\n",
        "\n",
        "def remove_half(df):\n",
        "\n",
        "  new_dict = {}\n",
        "\n",
        "  for idx in tqdm(range(0,len(df))):\n",
        "\n",
        "    text_split = df['Text'][idx].split()\n",
        "    x = round(len(text_split)/2)\n",
        "    new_text_arr_1 = text_split[:x]\n",
        "    new_text_arr_2 = text_split[x:]\n",
        "    new_text_1 = \" \".join(new_text_arr_1)\n",
        "    new_text_2 = \" \".join(new_text_arr_2)\n",
        "    part_to_remove = random.choices((0,1))\n",
        "    if(part_to_remove==0):\n",
        "      new_dict[idx] = new_text_1, df['Label'][idx], df['Author-short'][idx]\n",
        "    else:\n",
        "      new_dict[idx] = new_text_2, df['Label'][idx], df['Author-short'][idx]\n",
        "\n",
        "  df_remove_half = pd.DataFrame.from_dict(new_dict,  orient='index', columns=['Text', 'Label', 'Author-short'])\n",
        "\n",
        "  return df_remove_half\n",
        "\n",
        "def remove_part(df):\n",
        "\n",
        "  new_dict_1 = {}\n",
        "  new_dict_2 = {}\n",
        "\n",
        "  for idx in tqdm(range(0,len(df))):\n",
        "\n",
        "    text_split = df['Text'][idx].split()\n",
        "    x = round(len(text_split)/3)\n",
        "    new_text_arr_1 = text_split[:x]\n",
        "    new_text_arr_2 = text_split[x:2*x]\n",
        "    new_text_arr_3 = text_split[2*x:]\n",
        "    new_text_1 = \" \".join(new_text_arr_1)\n",
        "    new_text_2 = \" \".join(new_text_arr_2)\n",
        "    new_text_3 = \" \".join(new_text_arr_3)\n",
        "    part_to_remove = random.choices((0,2))\n",
        "    if(part_to_remove==0):\n",
        "      new_dict_1[idx] = new_text_2, df['Label'][idx], df['Author-short'][idx]\n",
        "      new_dict_2[idx] = new_text_3, df['Label'][idx], df['Author-short'][idx]\n",
        "    if(part_to_remove==1):\n",
        "      new_dict_1[idx] = new_text_1, df['Label'][idx], df['Author-short'][idx]\n",
        "      new_dict_2[idx] = new_text_3, df['Label'][idx], df['Author-short'][idx]\n",
        "    else:\n",
        "      new_dict_1[idx] = new_text_1, df['Label'][idx], df['Author-short'][idx]\n",
        "      new_dict_2[idx] = new_text_2, df['Label'][idx], df['Author-short'][idx]\n",
        "\n",
        "\n",
        "\n",
        "  df_new_1 = pd.DataFrame.from_dict(new_dict_1, orient='index', columns=['Text', 'Label', 'Author-short'])\n",
        "  df_new_2 = pd.DataFrame.from_dict(new_dict_2, orient='index', columns=['Text', 'Label', 'Author-short'])\n",
        "\n",
        "  result = pd.concat([df_new_1, df_new_2])\n",
        "  df_remove_part = result.reset_index(drop=True)\n",
        "  df_remove_part\n",
        "\n",
        "  return df_remove_part"
      ],
      "metadata": {
        "id": "lBDAreY9R-fY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(df_outer, split_size):\n",
        "  df = df_outer.copy(deep=True)\n",
        "  df_train = pd.DataFrame()\n",
        "  df_test = pd.DataFrame()\n",
        "  idxs_to_drop = []\n",
        "  idxs_for_test = []\n",
        "\n",
        "  labels = df[\"Label\"].unique()\n",
        "  y = df['Label']\n",
        "  if len(df['Label'].unique()) < 8:\n",
        "    y = df ['Label'].factorize()[0]\n",
        "\n",
        "\n",
        "  for n, label in enumerate(labels):\n",
        "    idxs_for_label, = np.where(y == label)\n",
        "    idxs_for_test = np.random.choice(idxs_for_label, size = split_size, replace=False)\n",
        "\n",
        "    for idx in idxs_for_test:\n",
        "      df_test = df_test.append(df.iloc[idx])\n",
        "      idxs_to_drop.append(idx)\n",
        "\n",
        "\n",
        "  for idx in range(0, len(df)):\n",
        "    if(idx not in idxs_to_drop):\n",
        "      df_train = df_train.append(df.iloc[idx])\n",
        "\n",
        "  df_train = df_train.reset_index(drop=True)\n",
        "  df_test = df_test.reset_index(drop=True)\n",
        "\n",
        "  return df_train, df_test"
      ],
      "metadata": {
        "id": "BPRWgrYLguuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_experiment(model, df, test_size, val_size, data_type, scheme, fraction, alpha, n_realizations, normalization,):\n",
        "\n",
        "  normalize = \"normalized.0\"\n",
        "  if(normalization==True):\n",
        "    normalize = \"normalized.1\"\n",
        "  model_name = model[0]\n",
        "\n",
        "  n_classes = len(df['Label'].unique())\n",
        "  CM = np.zeros((n_classes, n_classes, n_realizations))\n",
        "  scores = []\n",
        "  epochs = 125\n",
        "  batch_size = 512\n",
        "  train_loss_realizations = np.zeros((epochs, n_realizations))\n",
        "  train_acc_realizations = np.zeros((epochs, n_realizations))\n",
        "  val_loss_realizations = np.zeros((epochs, n_realizations))\n",
        "  val_acc_realizations = np.zeros((epochs, n_realizations))\n",
        "  for n in range(0, n_realizations):\n",
        "    # podział danych na dane testowe oraz treningowe i validacyjne. Dane testowe i validacyjne poddaję embedingowi i tworzę dla nich X, y\n",
        "    df_train_and_val, df_test = split_data(df, test_size)\n",
        "    df_train_to_augment, df_val = split_data(df_train_and_val, val_size)\n",
        "\n",
        "    embed_test = make_embedding(df_test, model)\n",
        "    df_test = pd.concat([df_test, embed_test['embedding']], axis=1)\n",
        "\n",
        "    embed_val = make_embedding(df_val, model)\n",
        "    df_val = pd.concat([df_val, embed_val['embedding']], axis=1)\n",
        "\n",
        "    X_test, y_test = get_X_y_test(df_test, normalization)\n",
        "    X_val, y_val = get_X_y_test(df_val, normalization)\n",
        "\n",
        "    # augmentacja danych w zbiorze treningowym\n",
        "    args = [fraction, alpha]\n",
        "    df_train = generate_data(model, df_train_to_augment, scheme, args)\n",
        "    print('Test: {} Val: {} Train: {}'.format(df_test.shape, df_val.shape, df_train.shape))\n",
        "\n",
        "    X_train, y_train = get_X_y_test(df_train, normalization)\n",
        "    y_train = to_categorical(y_train)\n",
        "    y_val = to_categorical(y_val)\n",
        "\n",
        "    input_size = X_train.shape[1]\n",
        "\n",
        "    model_NN = Sequential([\n",
        "      Dense(input_size, input_dim=input_size, activation='relu'),\n",
        "      Dense(2*input_size, activation='relu'),\n",
        "      Dense(4*input_size, activation='relu'),\n",
        "      Dropout(0.2),\n",
        "      Dense(n_classes, activation='softmax')\n",
        "    ])\n",
        "    opt = keras.optimizers.Adam(learning_rate=1e-3)\n",
        "    model_NN.compile(loss='categorical_crossentropy', optimizer=opt, metrics='accuracy')\n",
        "    history = model_NN.fit(X_train, y_train,\n",
        "            batch_size=batch_size, epochs=epochs,verbose=0,\n",
        "            validation_data=(X_val, y_val))\n",
        "\n",
        "    train_loss_realizations[:,n] = history.history['loss']\n",
        "    val_loss_realizations[:,n] = history.history['val_loss']\n",
        "    train_acc_realizations[:,n] = history.history['accuracy']\n",
        "    val_acc_realizations[:,n] = history.history['val_accuracy']\n",
        "\n",
        "    classes = print_classes(df_test)\n",
        "\n",
        "    score, CM[:,:,n] = get_confusion_matrix(model_NN, X_test, y_test, 'neural_network', data_type, classes)\n",
        "    scores.append(score)\n",
        "\n",
        "# dane do uśrednionej krzywej uczenia\n",
        "  val_loss_mean = np.mean(val_loss_realizations, axis=1)\n",
        "  train_loss_mean = np.mean(train_loss_realizations, axis=1)\n",
        "  val_acc_mean = np.mean(val_acc_realizations, axis=1)\n",
        "  train_acc_mean = np.mean(train_acc_realizations, axis=1)\n",
        "\n",
        "  val_loss_std = np.std(val_loss_realizations, axis=1)\n",
        "  train_loss_std = np.std(train_loss_realizations, axis=1)\n",
        "  val_acc_std = np.std(val_acc_realizations, axis=1)\n",
        "  train_acc_std = np.std(train_acc_realizations, axis=1)\n",
        "\n",
        "  dict_history = {\"val_loss_mean\":val_loss_mean,\n",
        "                  \"train_loss_mean\":train_loss_mean,\n",
        "                  \"val_acc_mean\":val_acc_mean,\n",
        "                  \"train_acc_mean\":train_acc_mean,\n",
        "                  \"val_loss_std\":val_loss_std,\n",
        "                  \"train_loss_std\":train_loss_std,\n",
        "                  \"val_acc_std\":val_acc_std,\n",
        "                  \"train_acc_std\":train_acc_std}\n",
        "\n",
        "# uśredniona krzywa uczenia\n",
        "  fontsize = 16\n",
        "  epoch_vec = np.arange(0,epochs)\n",
        "  fig, ax = plt.subplots(1,2,figsize = (12, 8))\n",
        "  clrs = sns.color_palette(\"flare\")\n",
        "  ax[1].set_ylim([0,2])\n",
        "  ax[1].plot(epoch_vec, train_loss_mean, label = \"train\")\n",
        "  ax[1].fill_between(epoch_vec, train_loss_mean - train_loss_std, train_loss_mean + train_loss_std, alpha = 0.3, facecolor=clrs[4] )\n",
        "  ax[1].plot(val_loss_mean,  label = \"val\")\n",
        "  ax[1].fill_between(epoch_vec, val_loss_mean - val_loss_std, val_loss_mean + val_loss_std, alpha = 0.3, facecolor=clrs[4] )\n",
        "\n",
        "  ax[0].plot(epoch_vec, train_acc_mean,  label = \"train\")\n",
        "  ax[0].fill_between(epoch_vec, train_acc_mean - train_acc_std, train_acc_mean + train_acc_std, alpha = 0.3, facecolor=clrs[4])\n",
        "  ax[0].plot(val_acc_mean,  label = \"val\")\n",
        "  ax[0].fill_between(epoch_vec, val_acc_mean - val_acc_std, val_acc_mean + val_acc_std, alpha = 0.3, facecolor=clrs[4] )\n",
        "\n",
        "\n",
        "  ax[1].set_xlabel(\"Traning epoch\", fontsize=fontsize)\n",
        "  ax[1].set_ylabel(\"Loss\", fontsize=fontsize)\n",
        "  ax[0].set_xlabel(\"Traning epoch\", fontsize=fontsize)\n",
        "  ax[0].set_ylabel(\"Accuracy\", fontsize=fontsize)\n",
        "\n",
        "  ax[1].legend( fontsize = fontsize)\n",
        "  ax[0].legend( fontsize = fontsize)\n",
        "  fig.suptitle('Learning curve | Data type: {}'.format(data_type))\n",
        "\n",
        "  fig.savefig('/content/figs/avgr_learning_curve_{}_{}_{}_scheme_{}_fraction_{}_alpha_{}.png'.format(model_name, data_type, normalize, scheme, fraction, alpha))\n",
        "  files.download('/content/figs/avgr_learning_curve_{}_{}_{}_scheme_{}_fraction_{}_alpha_{}.png'.format(model_name, data_type, normalize, scheme, fraction, alpha))\n",
        "\n",
        "# uśredniona macierz konfuzji\n",
        "  CM_avrg = np.zeros((n_classes,n_classes))\n",
        "  CM_std = np.zeros((n_classes,n_classes))\n",
        "  score_avrg = np.mean(scores)\n",
        "\n",
        "  for i in range(0,n_classes):\n",
        "    for j in range(0,n_classes):\n",
        "      CM_avrg[i,j] = np.mean(CM[i,j,:])\n",
        "      CM_std[i,j] = np.std(CM[i,j,:])\n",
        "\n",
        "  classes = print_classes(df)\n",
        "  cls = []\n",
        "  for k in classes.keys():\n",
        "    cls.append(k)\n",
        "\n",
        "  tick_marks = np.arange(len(cls))\n",
        "  cms = {\"Average\": CM_avrg, \"Std\": CM_std}\n",
        "\n",
        "\n",
        "  fig2, axes = plt.subplots(1, 2, figsize=(20,10), sharey='row')\n",
        "\n",
        "  for i, (key, cm) in enumerate(cms.items()):\n",
        "\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=cls)\n",
        "    disp.plot(ax=axes[i], xticks_rotation=45)\n",
        "    disp.ax_.set_title(\"{} | Model: Neural Network | Data type: {} | Acc: {}\".format(key, data_type, round(score_avrg,2)))\n",
        "    disp.im_.colorbar.remove()\n",
        "    disp.ax_.set_xlabel('')\n",
        "    disp.ax_.set_ylabel('')\n",
        "\n",
        "\n",
        "  fig2.text(0.40, 0.1, 'Predicted label', ha='left')\n",
        "  plt.subplots_adjust(wspace=0.40, hspace=0.1)\n",
        "\n",
        "  fig2.colorbar(disp.im_, ax=axes)\n",
        "  plt.show()\n",
        "\n",
        "  plt.gcf().set_size_inches(10, 5)\n",
        "  fig2.savefig('/content/figs/avrg_neural_network_{}_{}_{}_scheme_{}_fraction_{}_alpha_{}.png'.format(model_name, data_type, normalize, scheme, fraction, alpha), dpi=200)\n",
        "  files.download('/content/figs/avrg_neural_network_{}_{}_{}_scheme_{}_fraction_{}_alpha_{}.png'.format(model_name, data_type, normalize, scheme, fraction, alpha))\n",
        "\n",
        "  return score_avrg"
      ],
      "metadata": {
        "id": "42ynTtdEQtKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_data(model, df_outer, scheme, args):\n",
        "\n",
        "  f, alpha = args\n",
        "  df = df_outer.copy(deep=True)\n",
        "  if(scheme==0):\n",
        "    df_new = cut_in_half(df)\n",
        "\n",
        "  if(scheme==1):\n",
        "    df_new = flip(df)\n",
        "\n",
        "  if(scheme==2):\n",
        "    df_new_1 = cut_in_half(df)\n",
        "    df_new_1 = flip(df_new_1)\n",
        "    df_new_2 = flip(df)\n",
        "    df_new = pd.concat([df_new_1, df_new_2]).reset_index(drop=True)\n",
        "\n",
        "  if(scheme==3):\n",
        "      df_new = remove_words(df, f)\n",
        "\n",
        "  if(scheme==4):\n",
        "      df_new_1 = cut_in_half(df)\n",
        "      df_new_1 = remove_words(df_new_1, f)\n",
        "      df_new_2 = remove_words(df, f)\n",
        "      df_new = pd.concat([df_new_1, df_new_2]).reset_index(drop=True)\n",
        "\n",
        "  if(scheme==5):\n",
        "      df_new_1 = remove_words(df, f)\n",
        "      df_new_1 = flip(df_new_1)\n",
        "      df_new_2 = remove_words(df, f)\n",
        "      df_new = pd.concat([df_new_1, df_new_2]).reset_index(drop=True)\n",
        "\n",
        "  if(scheme==6):\n",
        "      df_new_1 = cut_in_half(df)\n",
        "      df_new_1 = flip(df_new_1)\n",
        "      df_new_2 = remove_words(df, f)\n",
        "      df_new_2 = flip(df_new_2)\n",
        "      df_new_3 = remove_words(df, f)\n",
        "      df_new = pd.concat([df_new_1, df_new_2, df_new_3]).reset_index(drop=True)\n",
        "\n",
        "  if(scheme==7):\n",
        "\n",
        "      df_new_1 = cut_in_half(df)\n",
        "      df_new_1 = remove_words(df_new_1, f)\n",
        "      df_new_2 = cut_in_half(df)\n",
        "      df_new_2 = remove_words(df_new_2, f)\n",
        "      df_new_2 = flip(df_new_2)\n",
        "      df_new_3 = remove_words(df, f)\n",
        "      df_new_3 = flip(df_new_3)\n",
        "      df_new_4 = remove_words(df, f)\n",
        "      df_new = pd.concat([df_new_1, df_new_2, df_new_3, df_new_4]).reset_index(drop=True)\n",
        "\n",
        "  if(scheme==8):\n",
        "\n",
        "      df_new_1 = cut_and_flip(df,1)\n",
        "      df_new_2 = cut_and_flip(df,2)\n",
        "      df_new_3 = cut_and_flip(df,3)\n",
        "      df_new_4 = remove_words(df,f)\n",
        "      df_new_4 = flip(df_new_4)\n",
        "      df_new_5 = remove_words(df,f)\n",
        "      df_new = pd.concat([df_new_1, df_new_2, df_new_3, df_new_4, df_new_5]).reset_index(drop=True)\n",
        "\n",
        "  if(scheme==9):\n",
        "      df_new_1 = cut_and_flip(df,1)\n",
        "      df_new_1 = remove_words(df_new_1,f)\n",
        "      df_new_2 = cut_and_flip(df,2)\n",
        "      df_new_2 = remove_words(df_new_2,f)\n",
        "      df_new_3 = cut_and_flip(df,3)\n",
        "      df_new_3 =remove_words(df_new_3,f)\n",
        "      df_new_4 = remove_words(df,f)\n",
        "      df_new_4 = flip(df_new_4)\n",
        "      df_new_5 = remove_words(df,f)\n",
        "      df_new = pd.concat([df_new_1, df_new_2, df_new_3, df_new_4, df_new_5]).reset_index(drop=True)\n",
        "\n",
        "  if(scheme==10):\n",
        "      df_new_1 = df\n",
        "      df_new_aug = df.sample(round(len(df)*alpha)).reset_index(drop=True)\n",
        "      df_new_2 = remove_words(df_new_aug, f)\n",
        "      df_new_3 = flip(df_new_aug)\n",
        "      df_new_4 = cut_in_half(df_new_aug)\n",
        "      df_new = pd.concat([df_new_1, df_new_2, df_new_3, df_new_4]).reset_index(drop=True)\n",
        "\n",
        "\n",
        "  if(scheme==11):\n",
        "      df_new_1 = df\n",
        "      df_new_aug = df.sample(frac=alpha,axis=0).reset_index(drop=True)\n",
        "      x = int(len(df_new_aug)/4)\n",
        "      df_new_aug_1 = df_new_aug.iloc[:x]\n",
        "      df_new_aug_2 = df_new_aug.iloc[x:2*x]\n",
        "      df_new_aug_3 = df_new_aug.iloc[2*x:3*x]\n",
        "      df_new_aug_4 = df_new_aug.iloc[3*x:]\n",
        "      df_new_2 = remove_words(df_new_aug_1.reset_index(drop=True), f)\n",
        "      df_new_3 = flip(df_new_aug_2.reset_index(drop=True))\n",
        "      #df_new_4 = swap_words(df_new_aug_3.reset_index(drop=True))\n",
        "      df_new_5 = cut_in_half(df_new_aug_4.reset_index(drop=True))\n",
        "      #df_new = pd.concat([df_new_1, df_new_2, df_new_3, df_new_4, df_new_5]).reset_index(drop=True)\n",
        "      df_new = pd.concat([df_new_1, df_new_2, df_new_3,   df_new_5]).reset_index(drop=True)\n",
        "\n",
        "  if(scheme==12):\n",
        "      df_new_1 = df\n",
        "      df_new_aug = df.sample(frac=alpha,axis=0).reset_index(drop=True)\n",
        "      df_new_2 = remove_words(df_new_aug.sample(frac = 0.5, axis=0).reset_index(drop=True), f)\n",
        "      df_new_3 = flip(df_new_aug.sample(frac = 0.5, axis=0).reset_index(drop=True))\n",
        "      #df_new_4 = swap_words(df_new_aug.sample(frac = 0.33, axis=0).reset_index(drop=True))\n",
        "      df_new = pd.concat([df_new_1, df_new_2, df_new_3]).reset_index(drop=True)\n",
        "\n",
        "\n",
        "  if(scheme==13):\n",
        "      df_new_1 = df\n",
        "\n",
        "      df_new_aug = df.sample(frac=alpha,axis=0).reset_index(drop=True)\n",
        "      df_new_2 = remove_words(df_new_aug.sample(frac = 0.5, axis=0).reset_index(drop=True), f)\n",
        "      #df_new_3 = flip(df_new_aug.sample(frac = 0.33, axis=0).reset_index(drop=True))\n",
        "      df_new_4 = swap_words(df_new_aug.sample(frac = 0.5, axis=0).reset_index(drop=True))\n",
        "      df_new = pd.concat([df_new_1, df_new_2,  df_new_4]).reset_index(drop=True)\n",
        "\n",
        "\n",
        "\n",
        "  if(scheme==14):\n",
        "      df_new_1 = df\n",
        "\n",
        "      df_new_aug = df.sample(frac=alpha,axis=0).reset_index(drop=True)\n",
        "     # df_new_2 = remove_words(df_new_aug.sample(frac = 0.33, axis=0).reset_index(drop=True), f)\n",
        "      df_new_3 = flip(df_new_aug.sample(frac = 0.5, axis=0).reset_index(drop=True))\n",
        "      df_new_4 = swap_words(df_new_aug.sample(frac = 0.5, axis=0).reset_index(drop=True))\n",
        "      df_new = pd.concat([df_new_1, df_new_3, df_new_4]).reset_index(drop=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  df_embed = make_embedding(df_new, model)\n",
        "  df_new = pd.concat([df_new, df_embed[\"embedding\"]], axis=1)\n",
        "  df_new = df_new.sample(frac = 1).reset_index(drop=True)\n",
        "\n",
        "  return df_new"
      ],
      "metadata": {
        "id": "VYJZ8dklSPte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "herbert_large = [\"Herbert-large\", HerbertTokenizer.from_pretrained(\"allegro/herbert-large-cased\"), RobertaModel.from_pretrained(\"allegro/herbert-large-cased\")]\n",
        "herbert_base = [\"Herbert-base\", HerbertTokenizer.from_pretrained(\"allegro/herbert-base-cased\"), RobertaModel.from_pretrained(\"allegro/herbert-base-cased\")]\n",
        "herbert_klej = [\"Herbert-klej\", HerbertTokenizer.from_pretrained(\"allegro/herbert-klej-cased-tokenizer-v1\"), RobertaModel.from_pretrained(\"allegro/herbert-klej-cased-v1\")]"
      ],
      "metadata": {
        "id": "psQ-y7-wRUcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_raw = pd.read_csv('/content/wiersze_do_BERT_Herbert_Miłosz.csv', \";\")\n",
        "df_raw .columns"
      ],
      "metadata": {
        "id": "Is8A9eO4RYXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_raw  = df_raw.drop(columns = ['Unnamed: 9', 'Unnamed: 10', 'Unnamed: 11'])\n",
        "df_raw.shape"
      ],
      "metadata": {
        "id": "a-NiIoQiRbQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_raw.iloc[400:]"
      ],
      "metadata": {
        "id": "sIWXKVbbY_cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_raw = df_raw.drop(df_raw.index[400:])\n",
        "df_raw"
      ],
      "metadata": {
        "id": "lyoxkvV6Ybgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_women = df_raw[200:].reset_index(drop=True)\n",
        "df_men = df_raw[:200].reset_index(drop=True)\n",
        "df_women = df_women.sample(frac = 1).reset_index(drop=True)\n",
        "df_men = df_men.sample(frac = 1).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "S_NXcd_cRm0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_orginal = pd.DataFrame\n",
        "df_orginal = pd.concat([df_raw[\"Text\"],df_raw[\"Label\"],df_raw[\"Author-short\"]], axis=1)\n",
        "df_orginal = df_orginal.sample(frac = 1).reset_index(drop=True)\n",
        "df_orginal"
      ],
      "metadata": {
        "id": "zgNS4unLRq13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df_orginal\n",
        "model = herbert_klej\n",
        "\n",
        "n_realizations  = 10\n",
        "test_size       = 16    # number of samples per class for testing. Maximum: 50\n",
        "val_size        = 8     # number of samples per class for validating. Maximum: 50\n",
        "\n",
        "fraction_vec = np.array([0.1])\n",
        "scheme_vec  = np.array([11])\n",
        "alpha_vec   = np.array([0, 0.2])\n",
        "\n",
        "accuracy_vs_scheme_vs_fraction = np.zeros((scheme_vec.shape[0], alpha_vec.shape[0]))\n",
        "dict_scores_all = []\n",
        "for idx_scheme, scheme in enumerate(scheme_vec):\n",
        "  for idx_alpha, alpha in enumerate(alpha_vec):\n",
        "    for idx_fraction, fraction in enumerate(fraction_vec):\n",
        "      score = make_experiment(model, df, test_size, val_size, 'all', scheme, fraction, alpha, n_realizations, normalization=False)\n",
        "      dict_scores = {\"scheme\"     : scheme,\n",
        "                     \"fraction\"   : fraction,\n",
        "                     \"alpha\"      : alpha,\n",
        "                     \"score\"      : score\n",
        "                    }\n",
        "      dict_scores_all.append(dict_scores)\n",
        "      accuracy_vs_scheme_vs_fraction[idx_scheme, idx_alpha] = score\n",
        "      print(\"Scheme: {} | Fraction: {} | Score: {} | Alpha: {}\".format(scheme, fraction, score, alpha))\n",
        "\n",
        "    print(\"Acc matrix: \")\n",
        "    print(accuracy_vs_scheme_vs_fraction)\n",
        "\n",
        "df_scores_all = pd.DataFrame(dict_scores_all)"
      ],
      "metadata": {
        "id": "PFfZpCoMhx9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuhIDhmnDeM0"
      },
      "outputs": [],
      "source": [
        "accuracy_vs_scheme_vs_fraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2cEaUNI3JMR"
      },
      "outputs": [],
      "source": [
        "disp = ConfusionMatrixDisplay(confusion_matrix=accuracy_vs_scheme_vs_fraction)\n",
        "disp.plot()\n",
        "disp.ax_.set_title(\"Scheme and fraction matrix\")\n",
        "disp.ax_.set_xlabel('Fraction')\n",
        "disp.ax_.set_ylabel('Scheme')\n",
        "tick_marks = np.arange(len(fractions))\n",
        "plt.xticks(tick_marks, fractions)\n",
        "plt.yticks(tick_marks, schemes)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2To4Q6E_qQx"
      },
      "outputs": [],
      "source": [
        "X, y, X_train, X_test, y_train, y_test,  X_val, y_val = get_X_y(df)\n",
        "y_train = to_categorical(y_train)\n",
        "y_val = to_categorical(y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_MxCb5g2ut3"
      },
      "outputs": [],
      "source": [
        "classes = print_classes(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtJgfHpP_K77"
      },
      "outputs": [],
      "source": [
        "input_size = 1024\n",
        "num_classes = len(classes)\n",
        "batch_size = 512\n",
        "epochs = 200\n",
        "\n",
        "model_NN = Sequential([\n",
        "    Dense(input_size, input_dim=input_size, activation='relu'),\n",
        "    Dense(2*input_size, activation='relu'),\n",
        "   # Dropout(0.1),\n",
        "    #Dense(2*input_size, activation='relu'),\n",
        "    #Dropout(0.2),\n",
        "    #Dense(2*input_size, activation='relu'),\n",
        "    # Dense(2*input_size, activation='relu'),\n",
        "    # Dropout(0.2),\n",
        "    # Dense(2*input_size, activation='relu'),\n",
        "    # Dense(2*input_size, activation='relu'),\n",
        "   #  Dropout(0.2),\n",
        "     #Dense(2*input_size, activation='relu'),\n",
        "     Dense(4*input_size, activation='relu'),\n",
        "     Dropout(0.2),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "  ])\n",
        "\n",
        "model_NN.compile(loss='categorical_crossentropy', optimizer='Adam', metrics='accuracy')\n",
        "#callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=15)\n",
        "history = model_NN.fit(X_train, y_train, #callbacks=[callback],\n",
        "          batch_size=batch_size, epochs=epochs, verbose=1,\n",
        "          validation_data=(X_val, y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXIPOhxeQSh5"
      },
      "outputs": [],
      "source": [
        "get_confusion_matrix(model_NN, X_test, y_test, 'neural_network', 'all', classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMwLMJcb3QEP"
      },
      "outputs": [],
      "source": [
        "make_experiment(df, 'all', '2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpFSJa7imp09"
      },
      "outputs": [],
      "source": [
        "val_loss_mean = np.mean(val_loss_realizations, axis=1)\n",
        "train_loss_mean = np.mean(train_loss_realizations, axis=1)\n",
        "val_acc_mean = np.mean(val_acc_realizations, axis=1)\n",
        "train_acc_mean = np.mean(train_acc_realizations, axis=1)\n",
        "\n",
        "val_loss_std = np.std(val_loss_realizations, axis=1)\n",
        "train_loss_std = np.std(train_loss_realizations, axis=1)\n",
        "val_acc_std = np.std(val_acc_realizations, axis=1)\n",
        "train_acc_std = np.std(train_acc_realizations, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-7rw9ieoGWo"
      },
      "outputs": [],
      "source": [
        "fontsize = 16\n",
        "data_type = \"all\"\n",
        "epoch_vec = np.arange(0,epochs)\n",
        "fig, ax = plt.subplots(1,2,figsize = (12, 8))\n",
        "clrs = sns.color_palette(\"flare\")\n",
        "ax[1].set_ylim([0,2])\n",
        "ax[1].plot(epoch_vec, train_loss_mean, label = \"train\")\n",
        "ax[1].fill_between(epoch_vec, train_loss_mean - train_loss_std, train_loss_mean + train_loss_std, alpha = 0.3, facecolor=clrs[4] )\n",
        "ax[1].plot(val_loss_mean,  label = \"val\")\n",
        "ax[1].fill_between(epoch_vec, val_loss_mean - val_loss_std, val_loss_mean + val_loss_std, alpha = 0.3, facecolor=clrs[4] )\n",
        "\n",
        "ax[0].plot(epoch_vec, train_acc_mean,  label = \"train\")\n",
        "ax[0].fill_between(epoch_vec, train_acc_mean - train_acc_std, train_acc_mean + train_acc_std, alpha = 0.3, facecolor=clrs[4])\n",
        "ax[0].plot(val_acc_mean,  label = \"val\")\n",
        "ax[0].fill_between(epoch_vec, val_acc_mean - val_acc_std, val_acc_mean + val_acc_std, alpha = 0.3, facecolor=clrs[4] )\n",
        "\n",
        "\n",
        "ax[1].set_xlabel(\"Traning epoch\", fontsize=fontsize)\n",
        "ax[1].set_ylabel(\"Loss\", fontsize=fontsize)\n",
        "ax[0].set_xlabel(\"Traning epoch\", fontsize=fontsize)\n",
        "ax[0].set_ylabel(\"Accuracy\", fontsize=fontsize)\n",
        "\n",
        "ax[1].legend( fontsize = fontsize)\n",
        "ax[0].legend( fontsize = fontsize)\n",
        "fig.suptitle('Learning curve | Data type: {}'.format(data_type))\n",
        "\n",
        "fig.savefig('/content/figs/avgr_learning_curve_{}_remove_cut_3_remove.png'.format(data_type))\n",
        "files.download('/content/figs/avgr_learning_curve_{}_remove_cut_3_remove.png'.format(data_type))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjQpdOBCvgO4"
      },
      "outputs": [],
      "source": [
        "CM_avrg = np.zeros((n_classes,n_classes))\n",
        "CM_std = np.zeros((n_classes,n_classes))\n",
        "score_avrg = np.mean(scores)\n",
        "\n",
        "for i in range(0,n_classes):\n",
        "  for j in range(0,n_classes):\n",
        "    CM_avrg[i,j] = np.mean(CM[i,j,:])\n",
        "    CM_std[i,j] = np.std(CM[i,j,:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiQ5rqEZeFQd"
      },
      "outputs": [],
      "source": [
        "classes = print_classes(df)\n",
        "cls = []\n",
        "for k in classes.keys():\n",
        "  cls.append(k)\n",
        "\n",
        "tick_marks = np.arange(4)\n",
        "cms = {\"Average\": CM_avrg, \"Std\": CM_std}\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(20,10), sharey='row')\n",
        "\n",
        "for i, (key, cm) in enumerate(cms.items()):\n",
        "\n",
        "  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=cls)\n",
        "  disp.plot(ax=axes[i], xticks_rotation=45)\n",
        "  disp.ax_.set_title(\"{} | Model: Neural Network | Data type: {} | Acc: {}\".format(key, data_type, round(score_avrg,2)))\n",
        "  disp.im_.colorbar.remove()\n",
        "  disp.ax_.set_xlabel('')\n",
        "  disp.ax_.set_ylabel('')\n",
        "\n",
        "\n",
        "fig.text(0.40, 0.1, 'Predicted label', ha='left')\n",
        "plt.subplots_adjust(wspace=0.40, hspace=0.1)\n",
        "\n",
        "fig.colorbar(disp.im_, ax=axes)\n",
        "plt.show()\n",
        "\n",
        "plt.gcf().set_size_inches(10, 5)\n",
        "fig.savefig('/content/figs/avrg_neural_network_{}_remove_cut_3_remove.png'.format(data_type), dpi=200)\n",
        "files.download('/content/figs/avrg_neural_network_{}_remove_cut_3_remove.png'.format(data_type))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akXpQ9wZL-SF"
      },
      "outputs": [],
      "source": [
        "scores, cm = get_confusion_matrix(model_NN, X_test, y_test, 'neural_network', 'all', classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xc_4aQ60Mvuj"
      },
      "outputs": [],
      "source": [
        "draw_learning_curve(history, 'all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyvcYxOIXy8_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1ZbLA058pBjTys-PhLyqITRtAi9ie_VKb",
      "authorship_tag": "ABX9TyNNM4fMPXe9WkjWZMmQKLTu"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}