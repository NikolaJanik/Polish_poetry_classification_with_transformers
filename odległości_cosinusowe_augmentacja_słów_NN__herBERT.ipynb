{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NikolaJanik/Polish_poetry_classification_with_transformers/blob/main/odleg%C5%82o%C5%9Bci_cosinusowe_augmentacja_s%C5%82%C3%B3w_NN__herBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_17sa8j9Cg4"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install sacremoses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLM5dKam9Vo6"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "import joblib\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from transformers import HerbertTokenizer, RobertaModel, AutoTokenizer, BertModel\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Y302eXuWka5"
      },
      "outputs": [],
      "source": [
        "os.mkdir(\"figs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtLD7hgHY8Kt"
      },
      "outputs": [],
      "source": [
        "def get_confusion_matrix(model, X_test, y_test, model_name, data_type, classes):\n",
        "\n",
        "  cls =[]\n",
        "  for k in classes.keys():\n",
        "    cls.append(k)\n",
        "\n",
        "  y_pred = model.predict(X_test)\n",
        "  pred_labels=[]\n",
        "  for idx in range(len(y_pred)):\n",
        "    pred_label = np.argmax(y_pred[idx])\n",
        "    pred_labels.append(pred_label)\n",
        "\n",
        "  true_labels = y_test\n",
        "  score = accuracy_score(true_labels, pred_labels)\n",
        "  cm = confusion_matrix(true_labels, pred_labels, normalize='true')\n",
        "\n",
        "  #disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "  #disp.plot()\n",
        "  #disp.ax_.set_title(\"Model: {} | Data type: {} |  Acc: {}\".format(model_name, data_type, score))\n",
        "\n",
        "  #if classes is not None:\n",
        "      #tick_marks = np.arange(len(cls))\n",
        "      #plt.xticks(tick_marks, cls, rotation=45)\n",
        "      #plt.yticks(tick_marks, cls, rotation=50)\n",
        "\n",
        "  #plt.gcf().set_size_inches(10, 10)\n",
        "  #plt.savefig('/content/figs/{}_{}.png'.format(model_name, data_type), dpi=200)\n",
        "  #files.download('/content/figs/{}_{}.png'.format(model_name, data_type))\n",
        "\n",
        "  return score, cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYBaBrbT9xTN"
      },
      "outputs": [],
      "source": [
        "def get_data_set(labels, df):\n",
        "  idxs = []\n",
        "  for label in labels:\n",
        "    idxs_for_label, = np.where(df['Label'] == label)\n",
        "    for idx in idxs_for_label:\n",
        "      idxs.append(idx)\n",
        "\n",
        "  new_df = df.iloc[idxs]\n",
        "  new_df = new_df.sample(frac = 1).reset_index(drop=True)\n",
        "  return new_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7MoiQjw-x5D"
      },
      "outputs": [],
      "source": [
        "def print_classes(df):\n",
        "  authors = {}\n",
        "  y = df['Label']\n",
        "  if len(df['Label'].unique()) < 8:\n",
        "    y = df ['Label'].factorize()[0]\n",
        "  num_classes = len(df['Label'].unique())\n",
        "  for label in range(0, num_classes):\n",
        "    i, = np.where(y == label)\n",
        "    authors['{}'.format(df['Author-short'][i[0]])] = label\n",
        "\n",
        "  return authors"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_embedding(df, model):\n",
        "\n",
        "  X_stack = []\n",
        "  model_name, tokenizer, model = model\n",
        "  embedded = {}\n",
        "  tokens = {}\n",
        "  num_idxs = df.shape[0]\n",
        "  for idx in tqdm(range(0,num_idxs)):\n",
        "    single_poem_input = df['Text'][idx]\n",
        "    inputs = tokenizer.batch_encode_plus([single_poem_input], max_length = 512, padding=\"longest\", add_special_tokens=True, return_tensors=\"pt\",)\n",
        "    single_poem_output = model(**inputs)\n",
        "    X_single_poem = single_poem_output[0][:,0,:].detach().numpy()\n",
        "    X_stack.append(X_single_poem[0])\n",
        "\n",
        "    embedded[idx] = X_single_poem[0], df['Label'][idx]\n",
        "\n",
        "  df_embedded = pd.DataFrame.from_dict(embedded,  orient='index', columns=['embedding', 'label'])\n",
        "\n",
        "  return df_embedded"
      ],
      "metadata": {
        "id": "24rzVy9CQTxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_data(X):\n",
        "\n",
        "  X_normalized = np.zeros((X.shape[0],X.shape[1]))\n",
        "\n",
        "  for idx in range(0,X.shape[0]):\n",
        "    X_normalized[idx,:] = (X[idx,:] - np.mean(X[idx,:]))/ np.std(X[idx,:])\n",
        "\n",
        "  return X_normalized"
      ],
      "metadata": {
        "id": "xls271BrQf1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_X_y(df, normalization=True):\n",
        "\n",
        "  X = np.stack(df['embedding'])\n",
        "  y = df['Label']\n",
        "  if(normalization==True):\n",
        "    X = normalize_data(X)\n",
        "\n",
        "  #jeśli jest mniej niż 8 klas:\n",
        "  if len(df['Label'].unique()) < 8:\n",
        "    y = df ['Label'].factorize()[0]\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "  X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
        "\n",
        "  print(X.shape)\n",
        "\n",
        "  return X, y, X_train, X_val, X_test, y_train, y_val, y_test"
      ],
      "metadata": {
        "id": "KGHRxlCPYnV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_X_y_train(df, normalization=True):\n",
        "\n",
        "  X = np.stack(df['embedding'])\n",
        "  y = df['Label'].values\n",
        "  if(normalization==True):\n",
        "    X = normalize_data(X)\n",
        "\n",
        "  #jeśli jest mniej niż 8 klas:\n",
        "  if len(df['Label'].unique()) < 8:\n",
        "    y = df ['Label'].factorize()[0]\n",
        "\n",
        "  X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "\n",
        "  print(X.shape)\n",
        "\n",
        "  return X, y, X_train, X_val, y_train, y_val"
      ],
      "metadata": {
        "id": "c_XQaNLoQh2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cosinus_predictions(X_train, y_train, X_test, y_test):\n",
        "\n",
        " # X_train_normalized = get_normalization(X_train)\n",
        " # X_test_normalized  = get_normalization(X_test)\n",
        "\n",
        "  train_labels = np.unique(y_train)\n",
        "  test_labels  = np.unique(y_test)\n",
        "  confusion_matrix = np.zeros((train_labels.shape[0], test_labels.shape[0]))\n",
        "\n",
        "  for idx_x_test, x_test in enumerate(X_test):\n",
        "    y_true = y_test[idx_x_test]\n",
        "\n",
        "    cos_distance_min = 10000\n",
        "    y_pred = 0\n",
        "    for idx_x_train, x_train in enumerate(X_train):\n",
        "      cos_distance = np.dot(x_test,x_train)\n",
        "      if(cos_distance < cos_distance_min):\n",
        "        cos_distance_min = cos_distance\n",
        "        y_pred = y_train[idx_x_train]\n",
        "\n",
        "    confusion_matrix[y_true, y_pred] = confusion_matrix[y_pred, y_true] + 1\n",
        "\n",
        "  for label in test_labels:\n",
        "    n_y_true = np.where(label == y_test)[0]\n",
        "    confusion_matrix[y_true, :] = confusion_matrix[y_true, :]/n_y_true*100\n",
        "\n",
        "  plt.imshow(confusion_matrix)\n",
        "  plt.colorbar()\n",
        "  return confusion_matrix\n"
      ],
      "metadata": {
        "id": "mTe67zPje9M0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_orginal = pd.DataFrame\n",
        "df_orginal = pd.concat([df_raw[\"Text\"],df_raw[\"Label\"],df_raw[\"Author-short\"]], axis=1)\n",
        "#df_orginal = df_orginal.sample(frac = 1).reset_index(drop=True)\n",
        "df_orginal"
      ],
      "metadata": {
        "id": "A3QHV80-RZLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed = make_embedding(df_orginal, herbert_klej)\n",
        "#embed = make_embedding(df_orginal, herbert_large)\n",
        "df_orginal = pd.concat([df_orginal, embed['embedding']], axis=1)\n",
        "df_orginal"
      ],
      "metadata": {
        "id": "X1UhyMSml8OA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_men = make_embedding(df_men, herbert_klej)\n",
        "df_men = pd.concat([df_men, embed_men['embedding']], axis=1)\n",
        "df_men"
      ],
      "metadata": {
        "id": "030Ec2CwcHb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_women = make_embedding(df_women, herbert_klej)\n",
        "df_women = pd.concat([df_women, embed_women['embedding']], axis=1)\n",
        "df_women"
      ],
      "metadata": {
        "id": "82BfcwmKcQPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df_women\n",
        "norm = False\n",
        "data_type = 'women'\n",
        "\n",
        "classes = print_classes(df)\n",
        "cls =[]\n",
        "for k in classes.keys():\n",
        "  cls.append(k)"
      ],
      "metadata": {
        "id": "Vj4on-6nn55v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_realizations = 10\n",
        "CM_aver = np.zeros((4,4))\n",
        "if norm==True:\n",
        "  normalize = 1\n",
        "else:\n",
        "  normalize = 0\n",
        "\n",
        "for n_realization in range(n_realizations):\n",
        "  X, y, X_train, X_test, y_train, y_test = get_X_y_train(df, normalization=False)\n",
        "\n",
        "  confusion_matrix = np.zeros((4,4))\n",
        "  train_labels = np.unique(y_train)\n",
        "  test_labels  = np.unique(y_test)\n",
        "\n",
        "  for idx_x_test in range(0,X_test.shape[0]):\n",
        "\n",
        "    x_test = X_test[idx_x_test,:]\n",
        "    y_true = int(y_test[idx_x_test])\n",
        "\n",
        "    distance_min = 10000\n",
        "    y_pred = 0\n",
        "\n",
        "    distance_from_x_train = np.zeros(X_train.shape[0])\n",
        "    for idx_x_train in range(X_train.shape[0]):\n",
        "      x_train = X_train[idx_x_train, :]\n",
        "      #distance = np.dot(x_test, x_train)\n",
        "      distance = np.sqrt(np.sum((x_train-x_test)**2)) #euclidean distance\n",
        "      distance_from_x_train[idx_x_train] = distance\n",
        "\n",
        "    idx_min_distance = np.argmin(distance_from_x_train)\n",
        "    y_pred = int(y_train[idx_min_distance])\n",
        "    #print(y_pred)\n",
        "      #print(y_true, y_pred)\n",
        "\n",
        "    confusion_matrix[y_true, y_pred] = confusion_matrix[ y_true, y_pred] + 1\n",
        "\n",
        "  for y_true in test_labels:\n",
        "\n",
        "    y_true = int(y_true)\n",
        "    confusion_matrix[y_true, :] = confusion_matrix[ y_true, :]/np.sum(confusion_matrix[y_true,:])\n",
        "\n",
        "  #print(n_realization, np.mean(np.diag(confusion_matrix)))\n",
        "  CM_aver = CM_aver + confusion_matrix\n",
        "CM_aver = CM_aver/n_realizations\n",
        "acc =  round(np.mean(np.diag(CM_aver)), 2)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=CM_aver)\n",
        "disp.plot()\n",
        "disp.ax_.set_title(\"Prediction by euclidean distance | Data type: {} |  Acc: {}\".format(data_type, acc))\n",
        "tick_marks = np.arange(len(cls))\n",
        "plt.xticks(tick_marks, cls, rotation=45)\n",
        "plt.yticks(tick_marks, cls, rotation=50)\n",
        "\n",
        "plt.gcf().set_size_inches(10, 10)\n",
        "plt.savefig('/content/figs/prediction_euclidean_distance_{}_normalize_{}.png'.format(data_type, normalize), dpi=200)\n",
        "#files.download('/content/figs/prediction_euclidean_distance_{}_normalize_{}.png'.format(data_type, normalize))\n",
        "\n",
        "\n",
        "\n",
        "print(np.mean(np.diag(CM_aver)))"
      ],
      "metadata": {
        "id": "xmZvmT8TchkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Distance from averaged vectors\n",
        "\n",
        "n_realizations = 100\n",
        "CM_aver = np.zeros((8,8))\n",
        "for n_realization in range(n_realizations):\n",
        "  X, y, X_train, X_test, y_train, y_test = get_X_y_train(df, normalization=True)\n",
        "\n",
        "  idx_class_0 = np.where(y_train == 0)[0]\n",
        "  idx_class_1 = np.where(y_train == 1)[0]\n",
        "  idx_class_2 = np.where(y_train == 2)[0]\n",
        "  idx_class_3 = np.where(y_train == 3)[0]\n",
        "  idx_class_4 = np.where(y_train == 4)[0]\n",
        "  idx_class_5 = np.where(y_train == 5)[0]\n",
        "  idx_class_6 = np.where(y_train == 6)[0]\n",
        "  idx_class_7 = np.where(y_train == 7)[0]\n",
        "\n",
        "  X_aver = np.zeros((8,X_train.shape[1]))\n",
        "\n",
        "  X_aver[0,:] = np.mean(X_train[idx_class_0,:],axis=0)\n",
        "  X_aver[1,:] = np.mean(X_train[idx_class_1,:],axis=0)\n",
        "  X_aver[2,:] = np.mean(X_train[idx_class_2,:],axis=0)\n",
        "  X_aver[3,:] = np.mean(X_train[idx_class_3,:],axis=0)\n",
        "  X_aver[4,:] = np.mean(X_train[idx_class_4,:],axis=0)\n",
        "  X_aver[5,:] = np.mean(X_train[idx_class_5,:],axis=0)\n",
        "  X_aver[6,:] = np.mean(X_train[idx_class_6,:],axis=0)\n",
        "  X_aver[7,:] = np.mean(X_train[idx_class_7,:],axis=0)\n",
        "\n",
        "\n",
        "  confusion_matrix = np.zeros((  8, 8))\n",
        "  train_labels = np.unique(y_train)\n",
        "  test_labels  = np.unique(y_test)\n",
        "\n",
        "  for idx_x_test in range(0,X_test.shape[0]):\n",
        "\n",
        "    x_test = X_test[idx_x_test,:]\n",
        "    y_true = int(y_test[idx_x_test])\n",
        "\n",
        "    distance_min = 10000\n",
        "    y_pred = 0\n",
        "\n",
        "    distance_from_x_train = np.zeros(X_aver.shape[0])\n",
        "    for idx_x_train in range(X_aver.shape[0]):\n",
        "      x_aver_train = X_aver[idx_x_train, :]\n",
        "      #distance = np.dot(x_test, x_train)\n",
        "      distance = np.sqrt(np.sum((x_aver_train-x_test)**2)) #euclidean distance\n",
        "      distance_from_x_train[idx_x_train] = distance\n",
        "\n",
        "    idx_min_distance = np.argmin(distance_from_x_train)\n",
        "    y_pred = int(y_train[idx_min_distance])\n",
        "    #print(y_pred)\n",
        "      #print(y_true, y_pred)\n",
        "\n",
        "    confusion_matrix[y_true, y_pred] = confusion_matrix[ y_true, y_pred] + 1\n",
        "\n",
        "  for y_true in test_labels:\n",
        "\n",
        "    y_true = int(y_true)\n",
        "    confusion_matrix[y_true, :] = confusion_matrix[ y_true, :]/np.sum(confusion_matrix[y_true,:])\n",
        "\n",
        "  print(n_realization, np.mean(np.diag(confusion_matrix)))\n",
        "  CM_aver = CM_aver + confusion_matrix\n",
        "CM_aver = CM_aver/n_realizations\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=CM_aver)\n",
        "disp.plot()\n",
        "\n",
        "tick_marks = np.arange(len(cls))\n",
        "plt.xticks(tick_marks, cls, rotation=45)\n",
        "plt.yticks(tick_marks, cls, rotation=50)\n",
        "plt.gcf().set_size_inches(10, 10)\n",
        "\n",
        "\n",
        "\n",
        "print(np.mean(np.diag(CM_aver)))"
      ],
      "metadata": {
        "id": "fiAjrosGZo6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_X_y_test(df, normalization=True):\n",
        "  X = np.stack(df['embedding'])\n",
        "  y = df['Label']\n",
        "  if(normalization == True):\n",
        "    X = normalize_data(X)\n",
        "\n",
        "  #jeśli jest mniej niż 8 klas:\n",
        "  if len(df['Label'].unique()) < 8:\n",
        "    y = df ['Label'].factorize()[0]\n",
        "\n",
        "  return X, y"
      ],
      "metadata": {
        "id": "BcvYHaP1QmLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "herbert_large = [\"Herbert-large\", HerbertTokenizer.from_pretrained(\"allegro/herbert-large-cased\"), RobertaModel.from_pretrained(\"allegro/herbert-large-cased\")]\n",
        "herbert_base = [\"Herbert-base\", HerbertTokenizer.from_pretrained(\"allegro/herbert-base-cased\"), RobertaModel.from_pretrained(\"allegro/herbert-base-cased\")]\n",
        "herbert_klej = [\"Herbert-klej\", HerbertTokenizer.from_pretrained(\"allegro/herbert-klej-cased-tokenizer-v1\"), RobertaModel.from_pretrained(\"allegro/herbert-klej-cased-v1\")]"
      ],
      "metadata": {
        "id": "psQ-y7-wRUcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_raw = pd.read_csv('/content/wiersze_do_BERT_Herbert_Miłosz.csv', \";\")\n",
        "df_raw .columns"
      ],
      "metadata": {
        "id": "Is8A9eO4RYXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_raw  = df_raw.drop(columns = ['Unnamed: 9', 'Unnamed: 10', 'Unnamed: 11'])\n",
        "df_raw.shape"
      ],
      "metadata": {
        "id": "a-NiIoQiRbQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_raw.iloc[400:]"
      ],
      "metadata": {
        "id": "sIWXKVbbY_cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_raw = df_raw.drop(df_raw.index[400:])\n",
        "df_raw"
      ],
      "metadata": {
        "id": "lyoxkvV6Ybgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_women = df_raw[200:].reset_index(drop=True)\n",
        "df_men = df_raw[:200].reset_index(drop=True)\n",
        "df_women = df_women.sample(frac = 1).reset_index(drop=True)\n",
        "df_men = df_men.sample(frac = 1).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "S_NXcd_cRm0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_orginal = pd.DataFrame\n",
        "df_orginal = pd.concat([df_raw[\"Text\"],df_raw[\"Label\"],df_raw[\"Author-short\"]], axis=1)\n",
        "df_orginal = df_orginal.sample(frac = 1).reset_index(drop=True)\n",
        "df_orginal"
      ],
      "metadata": {
        "id": "zgNS4unLRq13"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1ZbLA058pBjTys-PhLyqITRtAi9ie_VKb",
      "authorship_tag": "ABX9TyMDRBTOouuw8A7UWxHjD5t6",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}