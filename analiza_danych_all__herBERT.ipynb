{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_17sa8j9Cg4"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install sacremoses"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall numba\n",
        "!pip install umap-learn\n",
        "!pip install -U numba"
      ],
      "metadata": {
        "id": "41ApK9nDFyra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLM5dKam9Vo6"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "import joblib\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from transformers import HerbertTokenizer, RobertaModel, AutoTokenizer, BertModel\n",
        "\n",
        "import umap\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYBaBrbT9xTN"
      },
      "outputs": [],
      "source": [
        "def get_data_set(labels, df):\n",
        "  idxs = []\n",
        "  for label in labels:\n",
        "    idxs_for_label, = np.where(df['Label'] == label)\n",
        "    for idx in idxs_for_label:\n",
        "      idxs.append(idx)\n",
        "\n",
        "  new_df = df.iloc[idxs]\n",
        "  new_df = new_df.sample(frac = 1).reset_index(drop=True)\n",
        "  return new_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7MoiQjw-x5D"
      },
      "outputs": [],
      "source": [
        "def print_classes(df):\n",
        "  y = df[\"Label\"]\n",
        "  authors = {}\n",
        "  num_classes = len(df['Label'].unique())\n",
        "  for label in range(0, num_classes):\n",
        "    i, = np.where(y == label)\n",
        "    authors['{}'.format(df['Author-short'][i[0]])] = label\n",
        "\n",
        "  return authors"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_tokens(df, model):\n",
        "  model_name, tokenizer, model = model\n",
        "  tokens = {}\n",
        "\n",
        "  df_tokens = pd.DataFrame()\n",
        "  tokenize = lambda sent: tokenizer.encode_plus(sent, max_length=512, padding='max_length', truncation=True)\n",
        "  df_tokens['tokens'] = df['Text'].map(tokenize)\n",
        "  df_tokens['input_ids'] = df_tokens['tokens'].map(lambda t: t['input_ids'] )\n",
        "  df_tokens['token_type_ids'] = df_tokens['tokens'].map(lambda t: t['token_type_ids'] )\n",
        "  df_tokens['attention_mask'] = df_tokens['tokens'].map(lambda t: t['attention_mask'] )\n",
        "\n",
        "\n",
        "  input_ids = np.stack(df_tokens['input_ids'])\n",
        "  token_type_ids = np.stack(df_tokens['token_type_ids'])\n",
        "  attention_mask = np.stack(df_tokens['attention_mask'])\n",
        "\n",
        "  inputs = {\"input_ids\":torch.tensor(input_ids),\"token_type_ids\":torch.tensor(token_type_ids),\"attention_mask\":torch.tensor(attention_mask)}\n",
        "\n",
        "  return df_tokens, inputs\n"
      ],
      "metadata": {
        "id": "wCyrtBSswp27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tokens, inputs = make_tokens(df_raw, herbert)"
      ],
      "metadata": {
        "id": "SksBWf7qz37H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model(**inputs)"
      ],
      "metadata": {
        "id": "S5cqibv0Jsuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dzielenie tokenów na 9 porcji\n",
        "X_stack = []\n",
        "embedded = {}\n",
        "model_name, tokenizer, model = herbert\n",
        "\n",
        "for idx in tqdm(range(0,400)):\n",
        "  x = 0\n",
        "\n",
        "  for i in range(0,9):\n",
        "\n",
        "    y = x+100\n",
        "    if(i>7):\n",
        "      y = 512\n",
        "\n",
        "    input_ids = np.stack(df_tokens[\"input_ids\"].iloc[idx:idx+1])\n",
        "    token_type_ids = np.stack(df_tokens[\"token_type_ids\"].iloc[idx:idx+1])\n",
        "    attention_mask = np.stack(df_tokens[\"attention_mask\"].loc[idx:idx+1])\n",
        "\n",
        "    input_ids = np.array([input_ids[0][x:y]])\n",
        "    token_type_ids = np.array([token_type_ids[0][x:y]])\n",
        "    attention_mask = np.array([attention_mask[0][x:y]])\n",
        "\n",
        "    x = x + 60\n",
        "\n",
        "    inputs = {\"input_ids\":torch.tensor(input_ids),\"token_type_ids\":torch.tensor(token_type_ids),\"attention_mask\":torch.tensor(attention_mask)}\n",
        "\n",
        "    single_poem_output = model(**inputs)\n",
        "    X_single_poem = single_poem_output[0][:,0,:].detach().numpy()\n",
        "    X_stack.append(X_single_poem[0])\n",
        "    embedded[idx,i] = X_single_poem[0], df_raw['Label'][idx]\n",
        "\n",
        "\n",
        "  df_embedded = pd.DataFrame.from_dict(embedded,  orient='index', columns=['{}_embedding'.format(model_name), 'label'])\n",
        "\n"
      ],
      "metadata": {
        "id": "h_CqPX27BleG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcxPj1XC98Y5"
      },
      "outputs": [],
      "source": [
        "def make_embedding(df, model):\n",
        "\n",
        "  X_stack = []\n",
        "  model_name, tokenizer, model = model\n",
        "  embedded = {}\n",
        "  tokens = {}\n",
        "  num_idxs = df.shape[0]\n",
        "  for idx in tqdm(range(0,num_idxs)):\n",
        "    single_poem_input = df['Text'][idx]\n",
        "    inputs = tokenizer.batch_encode_plus([single_poem_input], max_length = 512, padding=\"longest\", add_special_tokens=True, return_tensors=\"pt\",)\n",
        "    single_poem_output = model(**inputs)\n",
        "    X_single_poem = single_poem_output[0][:,0,:].detach().numpy()\n",
        "    X_stack.append(X_single_poem[0])\n",
        "\n",
        "    embedded[idx] = X_single_poem[0], df['Label'][idx]\n",
        "\n",
        "  df_embedded = pd.DataFrame.from_dict(embedded,  orient='index', columns=['{}_embedding'.format(model_name), 'label'])\n",
        "\n",
        "  return df_embedded"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_X_y(df):\n",
        "\n",
        "  X = np.stack(df['Herbert_embedding'])\n",
        "  y = df['Label']\n",
        "\n",
        "  #jeśli jest mniej niż 8 klas:\n",
        "  if len(df['Label'].unique()) < 8:\n",
        "    y = df ['Label'].factorize()[0]\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "  X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
        "\n",
        "  print(X.shape)\n",
        "\n",
        "  return X, y, X_train, X_test, y_train, y_test,  X_val, y_val"
      ],
      "metadata": {
        "id": "vBgmJ_f_FFbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_data(X):\n",
        "\n",
        "  X_normalized = np.zeros((X.shape[0],X.shape[1]))\n",
        "\n",
        "  for idx in range(0,400):\n",
        "    X_normalized[idx,:] = (X[idx,:] - np.mean(X[idx,:]))/ np.std(X[idx,:])\n",
        "\n",
        "  return X_normalized"
      ],
      "metadata": {
        "id": "DuSBYI_0h106"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3oYyP5gm9uGe"
      },
      "outputs": [],
      "source": [
        "herbert = [\"Herbert\", HerbertTokenizer.from_pretrained(\"allegro/herbert-large-cased\"), RobertaModel.from_pretrained(\"allegro/herbert-large-cased\")]\n",
        "#bert = [\"Bert\", AutoTokenizer.from_pretrained(\"bert-base-uncased\"), BertModel.from_pretrained(\"bert-base-uncased\")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_EyIJ1K9kuf"
      },
      "outputs": [],
      "source": [
        "df_raw = pd.read_csv('/content/drive/MyDrive/wiersze_do_BERT_light.csv', \";\")\n",
        "df_raw.columns\n",
        "df_raw.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_BeforeEbedding = pd.DataFrame\n",
        "df_BeforeEbedding = pd.concat([df_raw['Text'],df_raw['Label'],df_raw['Author-short']], axis=1)\n",
        "df_BeforeEbedding"
      ],
      "metadata": {
        "id": "XX43L8Oy8xm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = make_embedding(df_BeforeEbedding, herbert)"
      ],
      "metadata": {
        "id": "yZ7v7okgBwcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_AfterEmbedding = pd.concat([df_BeforeEbedding, embedding['Herbert_embedding']], axis=1)\n",
        "df_AfterEmbedding"
      ],
      "metadata": {
        "id": "-YOnYa3DzHTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = print_classes(df_AfterEmbedding)\n",
        "classes"
      ],
      "metadata": {
        "id": "QYCINTOqJKZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y, _,_,_,_,_,_ = get_X_y(df_AfterEmbedding)"
      ],
      "metadata": {
        "id": "moPokgr2J3AT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Umap (data before normalization)**"
      ],
      "metadata": {
        "id": "6KldKb5Pye9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Umap\n",
        "df_umap = pd.DataFrame()\n",
        "df_umap[\"y\"] = df_AfterEmbedding['Author-short']\n",
        "data_type = 'all'\n",
        "num_classes = len(classes)\n",
        "n_neighbors = [10]\n",
        "min_distnces= [0.1]\n",
        "n_components=2\n",
        "metric='euclidean'\n",
        "\n",
        "for n_neighbor in n_neighbors:\n",
        "  for min_dist in min_distnces:\n",
        "    reducer = umap.UMAP(n_neighbors=n_neighbor,\n",
        "            min_dist=min_dist,\n",
        "            n_components=n_components,\n",
        "            metric=metric)\n",
        "    scaled_X = StandardScaler().fit_transform(X)\n",
        "    embed = reducer.fit_transform(scaled_X)\n",
        "\n",
        "    df_umap[\"comp-1\"] = embed[:,0]\n",
        "    df_umap[\"comp-2\"] = embed[:,1]\n",
        "\n",
        "    plt.figure(figsize = (10,10))\n",
        "    sns.scatterplot(x=\"comp-1\", y=\"comp-2\", hue=df_umap.y.tolist(),\n",
        "                   palette=sns.color_palette(\"Set2\", num_classes),\n",
        "                   data=df_umap, s=100).set(title=\"Poems data Umap projection | Data type: {} | N_neighbors: {} | Distance: {}\".format(data_type, n_neighbor, min_dist))\n",
        "\n",
        "\n",
        "plt.savefig('/content/figs/umap_{}.png'.format(data_type))\n",
        "files.download('/content/figs/umap_{}.png'.format(data_type))"
      ],
      "metadata": {
        "id": "16AVZy-nIuPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3D PCA (data before normalization)**"
      ],
      "metadata": {
        "id": "8rbVND0jys6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3D PCA\n",
        "data_type = 'all'\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X)\n",
        "scaled_X = scaler.transform(X)\n",
        "pca = PCA(n_components=3)\n",
        "pca.fit(scaled_X)\n",
        "pca_X = pca.transform(scaled_X)"
      ],
      "metadata": {
        "id": "cElpqKjBTpoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xax = pca_X[:,0]\n",
        "Yax = pca_X[:,1]\n",
        "Zax = pca_X[:,2]\n",
        "\n",
        "cdict = {0:'cyan',1:'red',2:'blue',3:'green',4:'yellow',5:'violet',6:'orange',7:'brown'}\n",
        "label = {0:'K. K. Baczyński',1:'A. Mickiewicz',2:'J. Kochanowki',3:'Cz. Miłosz',4:'W. Szymborska',5:'H. Poświatowska',6:'M. P. Jasnorzewska',7:'E. Lipska'}\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(14,9))\n",
        "ax = fig.add_subplot(111,\n",
        "                     projection='3d')\n",
        "\n",
        "for l in np.unique(y):\n",
        " ix=np.where(y==l)\n",
        " ax.scatter(Xax[ix],\n",
        "            Yax[ix],\n",
        "            Zax[ix],\n",
        "            c=cdict[l],\n",
        "            s=60,\n",
        "           label=label[l])\n",
        "\n",
        "ax.set_xlabel(\"PC1\",\n",
        "              fontsize=12)\n",
        "ax.set_ylabel(\"PC2\",\n",
        "              fontsize=12)\n",
        "ax.set_zlabel(\"PC3\",\n",
        "              fontsize=12)\n",
        "\n",
        "ax.view_init(30, 140)\n",
        "ax.legend()\n",
        "plt.title(\"Poems data 3D PCA projection | Data type: {}\".format(data_type))\n",
        "plt.show()\n",
        "\n",
        "plt.savefig('/content/figs/pca_3D_{}.png'.format(data_type))\n",
        "files.download('/content/figs/pca_3D_{}.png'.format(data_type))"
      ],
      "metadata": {
        "id": "BR_9d--PNrF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Normalization**"
      ],
      "metadata": {
        "id": "Y-ALxRztyWy6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_normalized = normalize_data(X)\n",
        "print(X_normalized)"
      ],
      "metadata": {
        "id": "Du7FTMYTTHpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Euclidean distance**"
      ],
      "metadata": {
        "id": "7xKyHI-Pyx9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distance_euclidean_X_normalized = np.zeros((X_normalized.shape[0], X_normalized.shape[0]))\n",
        "distance_cosinus_X_normalized = np.zeros((X_normalized.shape[0], X_normalized.shape[0]))\n",
        "\n",
        "for i in range(0,distance_euclidean_X_normalized.shape[0]):\n",
        "  x_normalized_i = X_normalized[i,:]\n",
        "  for j in range(0,distance_euclidean_X_normalized.shape[0]):\n",
        "    x_normalized_j = X_normalized[j,:]\n",
        "    distance_euclidean_X_normalized[i,j] = np.sqrt(np.sum( np.abs(x_normalized_i - x_normalized_j)**2))\n",
        "    distance_cosinus_X_normalized[i,j] = np.dot(x_normalized_i, x_normalized_j)/1024\n"
      ],
      "metadata": {
        "id": "tQK1GLJEV4JO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distances = {\"Euclidean distance\":distance_euclidean_X_normalized, \"Cosinus distance\":distance_cosinus_X_normalized}\n",
        "data_type = \"all\"\n",
        "fig, ax = plt.subplots(1,2, figsize=(20,10), sharey='row')\n",
        "im = ax[0].imshow(distance_euclidean_X_normalized)\n",
        "ax[0].set_title('Euclidean distance', fontsize=20)\n",
        "im2 = ax[1].imshow(distance_cosinus_X_normalized)\n",
        "ax[1].set_title('Cosinus distance', fontsize=20)\n",
        "fig.colorbar(im, ax=ax[0])\n",
        "fig.colorbar(im2, ax=ax[1])\n",
        "\n",
        "fig.savefig('/content/figs/euclidean_and_cosinus.png')\n",
        "files.download('/content/figs/euclidean_and_cosinus.png')\n"
      ],
      "metadata": {
        "id": "bOt8DITYYPBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "M = np.zeros((400,400))\n",
        "for i in range(0,400):\n",
        "  x_i = embedded_all['Herbert_embedding'][i]\n",
        "  for j in range(0,400):\n",
        "    x_j = embedded_all['Herbert_embedding'][j]\n",
        "\n",
        "    M[i,j] = np.sqrt(np.sum( np.abs(x_i - x_j)**2))"
      ],
      "metadata": {
        "id": "CDWe-981W9aE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(M)\n",
        "plt.colorbar()\n",
        "plt.title('Distance between vectors | data type: all')\n",
        "\n",
        "\n",
        "plt.savefig('/content/odległość_wektorów_po_ombeddingu_all.png')\n",
        "files.download('/content/odległość_wektorów_po_ombeddingu_all.png')"
      ],
      "metadata": {
        "id": "juUA0XvJYWbw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1sXz1c0udWOaxJgXJJk1p94EQBJ1YCS5V",
      "authorship_tag": "ABX9TyOo9nzOq6hmartP3xrFLY/n"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}