{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NikolaJanik/Polish_poetry_classification_with_transformers/blob/main/herBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Install Dependencies (for Colab)"
      ],
      "metadata": {
        "id": "CGmkx-1RlX2m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_17sa8j9Cg4"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install sacremoses"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Imports"
      ],
      "metadata": {
        "id": "WNouqB_Plhcb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLM5dKam9Vo6"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "import joblib\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from transformers import HerbertTokenizer, RobertaModel, AutoTokenizer, BertModel\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Load and clean data"
      ],
      "metadata": {
        "id": "gjMPFZZtnRbC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_raw = pd.read_csv('/content/drive/MyDrive/wiersze_do_BERT_light.csv', \";\")\n",
        "df_raw  = df_raw .drop(columns = ['Unnamed: 9', 'Unnamed: 10', 'Unnamed: 11'])\n",
        "df_women = df_raw[200:].reset_index(drop=True)\n",
        "df_men = df_raw[:200].reset_index(drop=True)"
      ],
      "metadata": {
        "id": "RRX47ki8oFfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_set(labels, df):\n",
        "  idxs = []\n",
        "  for label in labels:\n",
        "    idxs_for_label, = np.where(df['Label'] == label)\n",
        "    for idx in idxs_for_label:\n",
        "      idxs.append(idx)\n",
        "\n",
        "  new_df = df.iloc[idxs]\n",
        "  new_df = new_df.sample(frac = 1).reset_index(drop=True)\n",
        "  return new_df"
      ],
      "metadata": {
        "id": "R6zQC9Aso1X-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [0,1,2,3,4,5,6,7]\n",
        "df = get_data_set(labels, df_raw)\n",
        "print(\"Number of classes: {}\".format(len(df['Label'].unique())))\n",
        "print(\"Shape of new data set: {}\".format(df.shape))"
      ],
      "metadata": {
        "id": "NpMuD6VyoFQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tokens, inputs = make_tokens(df_raw, herbert)\n",
        "input_ids = np.stack(df_tokens['input_ids'][1:2])\n",
        "token_type_ids = np.stack(df_tokens['token_type_ids'][1:2])\n",
        "attention_mask = np.stack(df_tokens['attention_mask'][1:2])\n",
        "\n",
        "inputs = {\"input_ids\":torch.tensor(input_ids),\"token_type_ids\":torch.tensor(token_type_ids),\"attention_mask\":torch.tensor(attention_mask)}\n",
        "outputs = model(**inputs)"
      ],
      "metadata": {
        "id": "P0RJmDt9nW_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_classes(df):\n",
        " # authors = {}\n",
        "  num_classes = len(df['label'].unique())\n",
        "#  for label in range(0, num_classes):\n",
        " #   i, = np.where(y == label)\n",
        "  #  authors['{}'.format(df['Author-short'][i[0]])] = label\n",
        "\n",
        "  return num_classes"
      ],
      "metadata": {
        "id": "FjZNPfDJsMxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = print_classes(df)\n",
        "classes"
      ],
      "metadata": {
        "id": "91HRIDDKsMkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Initialize HerBERT Model"
      ],
      "metadata": {
        "id": "roArFLGvn7WS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "herbert = [\"Herbert\", HerbertTokenizer.from_pretrained(\"allegro/herbert-large-cased\"), RobertaModel.from_pretrained(\"allegro/herbert-large-cased\")]\n",
        "bert = [\"Bert\", AutoTokenizer.from_pretrained(\"bert-base-uncased\"), BertModel.from_pretrained(\"bert-base-uncased\")]"
      ],
      "metadata": {
        "id": "Ik9_T2NOn8M4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Generate Embeddings"
      ],
      "metadata": {
        "id": "gk4233F4o-qX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_embedding(df, model):\n",
        "\n",
        "  X_stack = []\n",
        "  model_name, tokenizer, model = model\n",
        "  embedded = {}\n",
        "  tokens = {}\n",
        "  num_idxs = df.shape[0]\n",
        "  for idx in tqdm(range(0,num_idxs)):\n",
        "    single_poem_input = df['Text'][idx]\n",
        "    inputs = tokenizer.batch_encode_plus([single_poem_input], max_length = 512, padding=\"longest\", add_special_tokens=True, return_tensors=\"pt\",)\n",
        "    single_poem_output = model(**inputs)\n",
        "    X_single_poem = single_poem_output[0][:,0,:].detach().numpy()\n",
        "    X_stack.append(X_single_poem[0])\n",
        "\n",
        "    embedded[idx] = X_single_poem[0], df['Label'][idx]\n",
        "\n",
        "  df_embedded = pd.DataFrame.from_dict(embedded,  orient='index', columns=['{}_embedding'.format(model_name), 'label'])\n",
        "\n",
        "  return df_embedded"
      ],
      "metadata": {
        "id": "_z-tDDyUpGkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = make_embedding(df_raw, herbert)\n",
        "df = df_embedded.sample(frac = 1).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "G1ugS5kxpP1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Prepare X and y"
      ],
      "metadata": {
        "id": "DYZp6M6opDL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_X_y(df):\n",
        "\n",
        "  X = np.stack(df['Herbert_embedding'])\n",
        "  y = df['label']\n",
        "\n",
        "  #jeśli jest mniej niż 8 klas:\n",
        "  if len(df['label'].unique()) < 8:\n",
        "    y = df ['label'].factorize()[0]\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "  X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
        "\n",
        "  print(X.shape)\n",
        "\n",
        "  return X, y, X_train, X_test, y_train, y_test,  X_val, y_val"
      ],
      "metadata": {
        "id": "LXnOhNDypG6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y, X_train, X_test, y_train, y_test,  X_val, y_val = get_X_y(df)"
      ],
      "metadata": {
        "id": "yX9CC6UTpjqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Run Machine Learning models"
      ],
      "metadata": {
        "id": "xK-CIhLNp-PB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_models(X_train, y_train, X_test, y_test, data_type, classes):\n",
        "\n",
        "  cls = []\n",
        "  for k in classes.keys():\n",
        "    cls.append(k)\n",
        "\n",
        "  models = [\n",
        "      [\"decision_tree\", DecisionTreeClassifier(max_depth=20)]\n",
        "      #[\"random_forest\", RandomForestClassifier(n_estimators=50, max_depth=10, random_state=42)],\n",
        "      #[\"xgboost\", xgb.XGBClassifier(n_estimators=100, random_state=0)],\n",
        "      #[\"lgbm\", lgb.LGBMClassifier(n_estimators=50, random_state=0, max_depth=10)]\n",
        "  ]\n",
        "\n",
        "\n",
        "  for model_name, model_clf in models:\n",
        "\n",
        "    scores = model_clf.fit(X_train, y_train)\n",
        "    y_pred = model_clf.predict(X_test)\n",
        "    num_classes = len(classes)\n",
        "    score = accuracy_score(y_test, y_pred)\n",
        "    cm = confusion_matrix(y_test, y_pred, normalize='true')\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model_clf.classes_)\n",
        "    disp.plot()\n",
        "    disp.ax_.set_title(\"Model: {} | Data type: {} | Acc: {}\".format(model_name, data_type, score))\n",
        "\n",
        "    if classes is not None:\n",
        "        tick_marks = np.arange(len(cls))\n",
        "        plt.xticks(tick_marks, cls, rotation=45)\n",
        "        plt.yticks(tick_marks, cls, rotation=50)\n",
        "\n",
        "    #plt.gcf().set_size_inches(10, 10)\n",
        "    #plt.savefig('/content/figs/{}_{}_{}_classes.png'.format(model_name, data_type, num_classes), dpi=200)\n",
        "    #files.download('/content/figs/{}_{}_{}_classes.png'.format(model_name, data_type, num_classes))\n",
        "\n",
        "  return score, cm"
      ],
      "metadata": {
        "id": "xwz4L91JqQQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_models(X_train, y_train, X_test, y_test, \"all\", classes)"
      ],
      "metadata": {
        "id": "FoLYAOMSridn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = print_classes(df)\n",
        "cls = []\n",
        "for k in classes.keys():\n",
        "  cls.append(k)\n",
        "model_name = \"decision_tree\"\n",
        "data_type = 'women'\n",
        "n_realizations = 20\n",
        "CM = np.zeros((len(cls),len(cls),n_realizations))\n",
        "scores = []\n",
        "\n",
        "for r in range(0,n_realizations):\n",
        "  X, y, X_train, X_test, y_train, y_test,  X_val, y_val = get_X_y(df)\n",
        "  classes = print_classes(df)\n",
        "\n",
        "  score, CM[:,:,r] = run_models(X_train, y_train, X_test, y_test, \"women\", classes)\n",
        "  scores.append(score)\n",
        "\n",
        "CM_avrg = np.zeros((n_classes,n_classes))\n",
        "CM_std = np.zeros((n_classes,n_classes))\n",
        "score_avrg = np.mean(scores)\n",
        "\n",
        "for i in range(0,n_classes):\n",
        "  for j in range(0,n_classes):\n",
        "    CM_avrg[i,j] = np.mean(CM[i,j,:])\n",
        "    CM_std[i,j] = np.std(CM[i,j,:])\n",
        "\n",
        "\n",
        "tick_marks = np.arange(4)\n",
        "cms = {\"Average\": CM_avrg, \"Std\": CM_std}\n",
        "\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(20,10), sharey='row')\n",
        "\n",
        "for i, (key, cm) in enumerate(cms.items()):\n",
        "\n",
        "  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=cls)\n",
        "  disp.plot(ax=axes[i], xticks_rotation=45)\n",
        "  disp.ax_.set_title(\"{} | Model: {} | Data type: {} | Acc: {}\". format(key, model_name, data_type, round(score_avrg, 2)))\n",
        "  disp.im_.colorbar.remove()\n",
        "  disp.ax_.set_xlabel('')\n",
        "  disp.ax_.set_ylabel('')\n",
        "\n",
        "\n",
        "fig.text(0.40, 0.1, 'Predicted label', ha='left')\n",
        "plt.subplots_adjust(wspace=0.40, hspace=0.1)\n",
        "\n",
        "fig.colorbar(disp.im_, ax=axes)\n",
        "plt.show()\n",
        "\n",
        "plt.gcf().set_size_inches(10, 5)\n",
        "fig.savefig('/content/figs/avrg_{}_{}.png'.format(model_name, data_type), dpi=200)\n",
        "files.download('/content/figs/avrg_{}_{}.png'.format(model_name, data_type))"
      ],
      "metadata": {
        "id": "TwlUFfZTF7mJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Run Neural Network model"
      ],
      "metadata": {
        "id": "aSubvqpHrvor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_confusion_matrix(model, X_test, y_test, model_name, data_type, classes):\n",
        "\n",
        "  cls =[]\n",
        "  for k in classes.keys():\n",
        "    cls.append(k)\n",
        "\n",
        "  y_pred = model.predict(X_test)\n",
        "  pred_labels=[]\n",
        "  for idx in range(len(y_pred)):\n",
        "    pred_label = np.argmax(y_pred[idx])\n",
        "    pred_labels.append(pred_label)\n",
        "\n",
        "  true_labels = y_test\n",
        "  score = accuracy_score(true_labels, pred_labels)\n",
        "  cm = confusion_matrix(true_labels, pred_labels, normalize='true')\n",
        "\n",
        "  disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "  disp.plot()\n",
        "  disp.ax_.set_title(\"Model: {} | Data type: {} |  Acc: {}\".format(model_name, data_type, num_classes, score))\n",
        "\n",
        "  if classes is not None:\n",
        "      tick_marks = np.arange(len(cls))\n",
        "      plt.xticks(tick_marks, cls, rotation=45)\n",
        "      plt.yticks(tick_marks, cls, rotation=50)\n",
        "\n",
        "  #plt.gcf().set_size_inches(10, 10)\n",
        "  #plt.savefig('/content/figs/{}_{}.png'.format(model_name, data_type), dpi=200)\n",
        "  #files.download('/content/figs/{}_{}.png'.format(model_name, data_type))\n",
        "\n",
        "  return score, cm"
      ],
      "metadata": {
        "id": "ryuzGXnEt65O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_learning_curve(history, data_type, key='accuracy'):\n",
        "\n",
        "  fig, ax = plt.subplots(1, 2, figsize=(12,6))\n",
        "  ax[0].plot(history.history[key])\n",
        "  ax[0].plot(history.history['val_'+ key] )\n",
        "  ax[0].set_ylabel(key.title())\n",
        "  ax[0].set_xlabel('Epoch')\n",
        "  ax[0].legend(['train', 'val'])\n",
        "\n",
        "  ax[1].plot(history.history['loss'])\n",
        "  ax[1].plot(history.history['val_loss'] )\n",
        " # ax[1].set_ylim([0,1])\n",
        "  ax[1].set_ylabel('loss'.title())\n",
        "  ax[1].set_xlabel('Epoch')\n",
        "  ax[1].legend(['train', 'val'])\n",
        "  fig.suptitle('Learning curve | Data type: {}'.format(data_type))\n",
        "  plt.show()\n",
        "  fig.savefig('/content/figs/learning_curve_{}.png'.format(data_type))\n",
        "  files.download('/content/figs/learning_curve_{}.png'.format(data_type))\n"
      ],
      "metadata": {
        "id": "m4ArnU9Wt6v1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GItxoGHg_GtJ"
      },
      "outputs": [],
      "source": [
        "y_train = to_categorical(y_train)\n",
        "y_val = to_categorical(y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtJgfHpP_K77"
      },
      "outputs": [],
      "source": [
        "input_size = 1024\n",
        "num_classes = 8\n",
        "batch_size = 512\n",
        "epochs = 200\n",
        "\n",
        "model_NN = Sequential([\n",
        "    Dense(input_size, input_dim=input_size, activation='relu'),\n",
        "    Dense(2*input_size, activation='relu'),\n",
        "   # Dropout(0.1),\n",
        "   # Dense(2*input_size, activation='relu'),\n",
        "    #Dropout(0.1),\n",
        "   # Dense(2*input_size, activation='relu'),\n",
        "    # Dense(2*input_size, activation='relu'),\n",
        "    # Dropout(0.2),\n",
        "    # Dense(2*input_size, activation='relu'),\n",
        "    # Dense(2*input_size, activation='relu'),\n",
        "   #  Dropout(0.2),\n",
        "   #  Dense(4*input_size, activation='relu'),\n",
        "     Dense(4*input_size, activation='relu'),\n",
        "     Dropout(0.2),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "  ])\n",
        "\n",
        "model_NN.compile(loss='categorical_crossentropy', optimizer='Adam', metrics='accuracy')\n",
        "#callback = keras.callbacks.EarlyStopping(monitor='loss', patience=25)\n",
        "history = model_NN.fit(X_train, y_train,\n",
        "          batch_size=batch_size, epochs=epochs, verbose=1,\n",
        "          validation_data=(X_val, y_val))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_realizations = 20\n",
        "n_classes = 8\n",
        "CM = np.zeros((n_classes, n_classes, n_realizations))\n",
        "scores = []\n",
        "train_loss_realizations = np.zeros((epochs, n_realizations))\n",
        "train_acc_realizations = np.zeros((epochs, n_realizations))\n",
        "val_loss_realizations = np.zeros((epochs, n_realizations))\n",
        "val_acc_realizations = np.zeros((epochs, n_realizations))\n",
        "for n in range(0, n_realizations):\n",
        "  X, y, X_train, X_test, y_train, y_test,  X_val, y_val = get_X_y(df)\n",
        "  y_train = to_categorical(y_train)\n",
        "  y_val = to_categorical(y_val)\n",
        "\n",
        "  model_NN = Sequential([\n",
        "    Dense(input_size, input_dim=input_size, activation='relu'),\n",
        "    Dense(2*input_size, activation='relu'),\n",
        "    Dense(4*input_size, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "  ])\n",
        "\n",
        "  model_NN.compile(loss='categorical_crossentropy', optimizer='Adam', metrics='accuracy')\n",
        "  history = model_NN.fit(X_train, y_train,\n",
        "          batch_size=batch_size, epochs=epochs,verbose=0,\n",
        "          validation_data=(X_val, y_val))\n",
        "\n",
        "  train_loss_realizations[:,n] = history.history['loss']\n",
        "  val_loss_realizations[:,n] = history.history['val_loss']\n",
        "  train_acc_realizations[:,n] = history.history['accuracy']\n",
        "  val_acc_realizations[:,n] = history.history['val_accuracy']\n",
        "\n",
        "  classes = print_classes(df)\n",
        "\n",
        "  score, CM[:,:,n] = get_confusion_matrix(model_NN, X_test, y_test, 'neural_network', 'all', classes)\n",
        "  scores.append(score)"
      ],
      "metadata": {
        "id": "qgjhaeFAcbX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_loss_mean = np.mean(val_loss_realizations, axis=1)\n",
        "train_loss_mean = np.mean(train_loss_realizations, axis=1)\n",
        "val_acc_mean = np.mean(val_acc_realizations, axis=1)\n",
        "train_acc_mean = np.mean(train_acc_realizations, axis=1)\n",
        "\n",
        "val_loss_std = np.std(val_loss_realizations, axis=1)\n",
        "train_loss_std = np.std(train_loss_realizations, axis=1)\n",
        "val_acc_std = np.std(val_acc_realizations, axis=1)\n",
        "train_acc_std = np.std(train_acc_realizations, axis=1)"
      ],
      "metadata": {
        "id": "ZpFSJa7imp09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fontsize = 16\n",
        "epoch_vec = np.arange(0,epochs)\n",
        "fig, ax = plt.subplots(1,2,figsize = (12, 8))\n",
        "clrs = sns.color_palette(\"flare\")\n",
        "ax[1].set_ylim([0,2])\n",
        "ax[1].plot(epoch_vec, train_loss_mean, label = \"train\")\n",
        "ax[1].fill_between(epoch_vec, train_loss_mean - train_loss_std, train_loss_mean + train_loss_std, alpha = 0.3, facecolor=clrs[4] )\n",
        "ax[1].plot(val_loss_mean,  label = \"val\")\n",
        "ax[1].fill_between(epoch_vec, val_loss_mean - val_loss_std, val_loss_mean + val_loss_std, alpha = 0.3, facecolor=clrs[4] )\n",
        "\n",
        "ax[0].plot(epoch_vec, train_acc_mean,  label = \"train\")\n",
        "ax[0].fill_between(epoch_vec, train_acc_mean - train_acc_std, train_acc_mean + train_acc_std, alpha = 0.3, facecolor=clrs[4])\n",
        "ax[0].plot(val_acc_mean,  label = \"val\")\n",
        "ax[0].fill_between(epoch_vec, val_acc_mean - val_acc_std, val_acc_mean + val_acc_std, alpha = 0.3, facecolor=clrs[4] )\n",
        "\n",
        "\n",
        "ax[1].set_xlabel(\"Traning epoch\", fontsize=fontsize)\n",
        "ax[1].set_ylabel(\"Loss\", fontsize=fontsize)\n",
        "ax[0].set_xlabel(\"Traning epoch\", fontsize=fontsize)\n",
        "ax[0].set_ylabel(\"Accuracy\", fontsize=fontsize)\n",
        "\n",
        "ax[1].legend( fontsize = fontsize)\n",
        "ax[0].legend( fontsize = fontsize)\n",
        "fig.suptitle('Learning curve | Data type: {}'.format(data_type))\n",
        "\n",
        "fig.savefig('/content/figs/avgr_learning_curve_{}.png'.format(data_type))\n",
        "files.download('/content/figs/avgr_learning_curve_{}.png'.format(data_type))"
      ],
      "metadata": {
        "id": "L-7rw9ieoGWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CM_avrg = np.zeros((n_classes,n_classes))\n",
        "CM_std = np.zeros((n_classes,n_classes))\n",
        "score_avrg = np.mean(scores)\n",
        "\n",
        "for i in range(0,n_classes):\n",
        "  for j in range(0,n_classes):\n",
        "    CM_avrg[i,j] = np.mean(CM[i,j,:])\n",
        "    CM_std[i,j] = np.std(CM[i,j,:])"
      ],
      "metadata": {
        "id": "JjQpdOBCvgO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = print_classes(df)\n",
        "cls = []\n",
        "for k in classes.keys():\n",
        "  cls.append(k)\n",
        "\n",
        "tick_marks = np.arange(4)\n",
        "cms = {\"Average\": CM_avrg, \"Std\": CM_std}\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(20,10), sharey='row')\n",
        "\n",
        "for i, (key, cm) in enumerate(cms.items()):\n",
        "\n",
        "  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=cls)\n",
        "  disp.plot(ax=axes[i], xticks_rotation=45)\n",
        "  disp.ax_.set_title(\"{} | Model: Neural Network | Data type: {} | Acc: {}\".format(key, data_type, round(score_avrg,2)))\n",
        "  disp.im_.colorbar.remove()\n",
        "  disp.ax_.set_xlabel('')\n",
        "  disp.ax_.set_ylabel('')\n",
        "\n",
        "\n",
        "fig.text(0.40, 0.1, 'Predicted label', ha='left')\n",
        "plt.subplots_adjust(wspace=0.40, hspace=0.1)\n",
        "\n",
        "fig.colorbar(disp.im_, ax=axes)\n",
        "plt.show()\n",
        "\n",
        "plt.gcf().set_size_inches(10, 5)\n",
        "fig.savefig('/content/figs/avrg_neural_network_{}.png'.format(data_type), dpi=200)\n",
        "files.download('/content/figs/avrg_neural_network_{}.png'.format(data_type))"
      ],
      "metadata": {
        "id": "eiQ5rqEZeFQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores, cm = get_confusion_matrix(model_NN, X_test, y_test, 'neural_network', 'all', classes)"
      ],
      "metadata": {
        "id": "akXpQ9wZL-SF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "draw_learning_curve(history, 'all')"
      ],
      "metadata": {
        "id": "xc_4aQ60Mvuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. I don't why do I have that"
      ],
      "metadata": {
        "id": "kcjc08FytSx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_tokens(df, model):\n",
        "  model_name, tokenizer, model = model\n",
        "  tokens = {}\n",
        "\n",
        "  df_tokens = pd.DataFrame()\n",
        "  tokenize = lambda sent: tokenizer.encode_plus(sent, max_length=512, padding='max_length', truncation=True)\n",
        "  df_tokens['tokens'] = df['Text'].map(tokenize)\n",
        "  df_tokens['input_ids'] = df_tokens['tokens'].map(lambda t: t['input_ids'] )\n",
        "  df_tokens['token_type_ids'] = df_tokens['tokens'].map(lambda t: t['token_type_ids'] )\n",
        "  df_tokens['attention_mask'] = df_tokens['tokens'].map(lambda t: t['attention_mask'] )\n",
        "\n",
        "\n",
        "  input_ids = np.stack(df_tokens['input_ids'])\n",
        "  token_type_ids = np.stack(df_tokens['token_type_ids'])\n",
        "  attention_mask = np.stack(df_tokens['attention_mask'])\n",
        "\n",
        "  inputs = {\"input_ids\":torch.tensor(input_ids),\"token_type_ids\":torch.tensor(token_type_ids),\"attention_mask\":torch.tensor(attention_mask)}\n",
        "\n",
        "  return df_tokens, inputs"
      ],
      "metadata": {
        "id": "LZziVpaUtzpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z = np.stack(df_tokens['input_ids'][2])\n",
        "z = np.stack(z[60:160])"
      ],
      "metadata": {
        "id": "PuUbHwQsts0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_stack = []\n",
        "embedded = {}\n",
        "model_name, tokenizer, model = herbert\n",
        "window_step = 30\n",
        "window_size = 60\n",
        "\n",
        "for idx in tqdm(range(0,400)):\n",
        "  x = 0\n",
        "\n",
        "  for i in range(0,3):\n",
        "\n",
        "    y = x + window_size\n",
        "    if(i>7):\n",
        "      y = 512\n",
        "\n",
        "    input_ids = np.stack(df_tokens[\"input_ids\"].iloc[idx:idx+1])\n",
        "    token_type_ids = np.stack(df_tokens[\"token_type_ids\"].iloc[idx:idx+1])\n",
        "    attention_mask = np.stack(df_tokens[\"attention_mask\"].loc[idx:idx+1])\n",
        "\n",
        "    input_ids = np.array([input_ids[0][x:y]])\n",
        "    token_type_ids = np.array([token_type_ids[0][x:y]])\n",
        "    attention_mask = np.array([attention_mask[0][x:y]])\n",
        "\n",
        "    x = x + window_step\n",
        "\n",
        "    inputs = {\"input_ids\":torch.tensor(input_ids),\"token_type_ids\":torch.tensor(token_type_ids),\"attention_mask\":torch.tensor(attention_mask)}\n",
        "\n",
        "    single_poem_output = model(**inputs)\n",
        "    X_single_poem = single_poem_output[0][:,0,:].detach().numpy()\n",
        "    X_stack.append(X_single_poem[0])\n",
        "    embedded[idx,i] = X_single_poem[0], df_raw['Label'][idx], df_raw['Author-short'][idx]\n",
        "\n",
        "\n",
        "  df_embedded = pd.DataFrame.from_dict(embedded,  orient='index', columns=['{}_embedding'.format(model_name), 'Label'])"
      ],
      "metadata": {
        "id": "P-WsjLvQtZBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.stack(df_embedded[\"Herbert_embedding\"])\n",
        "input_ids = np.stack(df_tokens[\"input_ids\"][0:1][0:100])\n",
        "input_ids =np.stack(np.pad(input_ids,[(0, 512-len(input_ids))], mode='constant', constant_values=1))"
      ],
      "metadata": {
        "id": "TWQiwZBotZ0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_length_mean = np.mean(df_raw['Words'])\n",
        "text_length_std = np.std(df_raw['Words'])\n",
        "text_length_median = np.median(df_raw['Words'])\n",
        "text_length_mean, text_length_std, text_length_median"
      ],
      "metadata": {
        "id": "NITpBqhbtZjM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1hH9Onh_kbPCdGpQ48jTGyi0n3Si_W6rx",
      "authorship_tag": "ABX9TyN2C7kRhEkRy8eFYtlwAkvF",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}